{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (19720, 18)\n",
      "test (29582, 16)\n",
      "CPU times: user 2.49 s, sys: 548 ms, total: 3.04 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "#import tensorflow as tf\n",
    "import MeCab \n",
    "import re\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib_venn import venn2\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option(\"display.precision\", 8)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "    \n",
    "\n",
    "\n",
    "# seed\n",
    "seed = 817\n",
    "seed_everything(seed)\n",
    "\n",
    "# load train test\n",
    "train = pd.read_csv('../../data/input/probspace/train_data.csv')\n",
    "train['y_bin'] = pd.cut(train['y'], [0, 10, 100,1000,10000,100000,1000000,10000000000], labels=[1,2,3,4,5,6,7])\n",
    "train['y_bin'] = train['y_bin'].astype(int)\n",
    "test = pd.read_csv('../../data/input/probspace/test_data.csv')\n",
    "df = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "print ('train',train.shape)\n",
    "print ('test',test.shape)\n",
    "df['comments_ratings'] = df['comments_disabled'].astype(str)+df['ratings_disabled'].astype(str)\n",
    "    \n",
    "for c in ['channelId','channelTitle','collection_date','description','tags','comments_disabled','ratings_disabled','comments_ratings']:\n",
    "    lbl = LabelEncoder()\n",
    "    df[c+'_encoder'] = lbl.fit_transform(df[c].astype(str))    \n",
    "    \n",
    "df[\"c_date\"] = \"20\" + df[\"collection_date\"]\n",
    "df[\"c_date\"] = pd.to_datetime(df[\"c_date\"], utc=True, format=\"%Y.%d.%m\")\n",
    "df[\"c_year\"] = df[\"c_date\"].dt.year\n",
    "df[\"c_month\"] = df[\"c_date\"].dt.month\n",
    "df[\"c_day\"] = df[\"c_date\"].dt.day\n",
    "df[\"c_dayofweek\"] = df[\"c_date\"].dt.dayofweek\n",
    "\n",
    "df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"],utc=True, format=\"%Y-%m-%d\")\n",
    "df[\"year\"] = df[\"publishedAt\"].dt.year\n",
    "df[\"month\"] = df[\"publishedAt\"].dt.month\n",
    "df[\"weekofyear\"] = df[\"publishedAt\"].dt.weekofyear\n",
    "df[\"day\"] = df[\"publishedAt\"].dt.day\n",
    "df[\"dayofweek\"] = df[\"publishedAt\"].dt.dayofweek\n",
    "df[\"hour\"] = df[\"publishedAt\"].dt.hour\n",
    "df[\"minute\"] = df[\"publishedAt\"].dt.minute    \n",
    "\n",
    "df['seconds_from_publish'] = (df['c_date'] - df['publishedAt']).dt.seconds\n",
    "df['days_from_publish'] = (df['c_date'] - df['publishedAt']).dt.days\n",
    "df['months_from_publish'] = (df['c_date'] - df['publishedAt']).dt.days // 30\n",
    "df['years_from_publish'] = (df['c_date'] - df['publishedAt']).dt.days // 365\n",
    "\n",
    "df['days_from_publish_start'] = (df['publishedAt'] - df['publishedAt'].min()).dt.days\n",
    "df['days_from_cdate_start'] = (df['c_date'] - df['c_date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 1.38 s, total: 1min 20s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation,TruncatedSVD\n",
    "from gensim.sklearn_api.ldamodel import LdaTransformer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "import unicodedata\n",
    "\n",
    "class MecabTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wakati = MeCab.Tagger('-Owakati')\n",
    "        self.wakati.parse('')\n",
    "\n",
    "    def tokenize(self, line):\n",
    "        txt = self.wakati.parse(line)\n",
    "        txt = txt.split()\n",
    "        return txt\n",
    "    \n",
    "    def mecab_tokenizer(self, line):\n",
    "        node = self.wakati.parseToNode(line)\n",
    "        keywords = []\n",
    "        while node:\n",
    "            if node.feature.split(\",\")[0] == \"名詞\" or node.feature.split(\",\")[0] == \"形容詞\":\n",
    "                keywords.append(node.surface)\n",
    "            node = node.next\n",
    "        return keywords    \n",
    "    \n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "html_tags = ['<p>', '</p>', '<table>', '</table>', '<tr>', '</tr>', '<ul>', '<ol>', '<dl>', '</ul>', '</ol>',\n",
    "             '</dl>', '<li>', '<dd>', '<dt>', '</li>', '</dd>', '</dt>', '<h1>', '</h1>',\n",
    "             '<br>', '<br/>', '<strong>', '</strong>', '<span>', '</span>', '<blockquote>', '</blockquote>',\n",
    "             '<pre>', '</pre>', '<div>', '</div>', '<h2>', '</h2>', '<h3>', '</h3>', '<h4>', '</h4>', '<h5>', '</h5>',\n",
    "             '<h6>', '</h6>', '<blck>', '<pr>', '<code>', '<th>', '</th>', '<td>', '</td>', '<em>', '</em>']\n",
    "\n",
    "empty_expressions = ['&lt;', '&gt;', '&amp;', '&nbsp;', \n",
    "                     '&emsp;', '&ndash;', '&mdash;', '&ensp;'\n",
    "                     '&quot;', '&#39;']\n",
    "\n",
    "other = ['span', 'style', 'href', 'input']\n",
    "\n",
    "\n",
    "def pre_preprocess(x):\n",
    "    return str(x).lower()\n",
    "\n",
    "def rm_spaces(text):\n",
    "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u2009', '\\u2028', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\u3000', '\\x10', '\\x7f', '\\x9d', '\\xad',\n",
    "              '\\x97', '\\x9c', '\\x8b', '\\x81', '\\x80', '\\x8c', '\\x85', '\\x92', '\\x88', '\\x8d', '\\x80', '\\x8e', '\\x9a', '\\x94', '\\xa0', \n",
    "              '\\x8f', '\\x82', '\\x8a', '\\x93', '\\x90', '\\x83', '\\x96', '\\x9b', '\\x9e', '\\x99', '\\x87', '\\x84', '\\x9f',\n",
    "             ]\n",
    "    for space in spaces:\n",
    "            text = text.replace(space, ' ')\n",
    "    return text\n",
    "\n",
    "def remove_urls(x):\n",
    "    x = re.sub(r'(https?://[a-zA-Z0-9.-]*)', r'', x)\n",
    "\n",
    "    # original\n",
    "    x = re.sub(r'(quote=\\w+\\s?\\w+;?\\w+)', r'', x)\n",
    "    return x\n",
    "\n",
    "def clean_html_tags(x, stop_words=[]):      \n",
    "    for r in html_tags:\n",
    "        x = x.replace(r, '')\n",
    "    for r in empty_expressions:\n",
    "        x = x.replace(r, ' ')\n",
    "    for r in stop_words:\n",
    "        x = x.replace(r, '')\n",
    "    return x\n",
    "\n",
    "def replace_num(text):\n",
    "    text = re.sub('[0-9]{5,}', '', text)\n",
    "    text = re.sub('[0-9]{4}', '', text)\n",
    "    text = re.sub('[0-9]{3}', '', text)\n",
    "    text = re.sub('[0-9]{2}', '', text)\n",
    "    return text\n",
    "\n",
    "def get_url_num(x):\n",
    "    pattern = \"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\"\n",
    "    urls = re.findall(pattern, x)\n",
    "    return len(urls)\n",
    "\n",
    "\n",
    "def clean_puncts(x):\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "#zenkaku = '０,１,２,３,４,５,６,７,８,９,（,）,＊,「,」,［,］,【,】,＜,＞,？,・,＃,＠,＄,％,＝'.split(',')\n",
    "#hankaku = '0,1,2,3,4,5,6,7,8,9,q,a,z,w,s,x,c,d,e,r,f,v,b,g,t,y,h,n,m,j,u,i,k,l,o,p'.split(',')\n",
    "\n",
    "def clean_text_jp(x):\n",
    "    x = x.replace('。', '')\n",
    "    x = x.replace('、', '')\n",
    "    x = x.replace('\\n', '') # 改行削除\n",
    "    x = x.replace('\\t', '') # タブ削除\n",
    "    x = x.replace('\\r', '')\n",
    "    x = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', x) \n",
    "    x = re.sub(r'\\[math\\]', ' LaTex math ', x) # LaTex削除\n",
    "    x = re.sub(r'\\[\\/math\\]', ' LaTex math ', x) # LaTex削除\n",
    "    x = re.sub(r'\\\\', ' LaTex ', x) # LaTex削除   \n",
    "    #for r in zenkaku+hankaku:\n",
    "    #    x = x.replace(str(r), '')\n",
    "    x = re.sub(' +', ' ', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = data.apply(lambda x: pre_preprocess(x))\n",
    "    data = data.apply(lambda x: rm_spaces(x))\n",
    "    data = data.apply(lambda x: remove_urls(x))\n",
    "    data = data.apply(lambda x: clean_puncts(x))\n",
    "    data = data.apply(lambda x: replace_num(x))\n",
    "    data = data.apply(lambda x: clean_html_tags(x, stop_words=other))\n",
    "    data = data.apply(lambda x: clean_text_jp(x))\n",
    "    return data    \n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def is_japanese(string):\n",
    "    for ch in string:\n",
    "        try:\n",
    "            name = unicodedata.name(ch) \n",
    "            if \"CJK UNIFIED\" in name \\\n",
    "            or \"HIRAGANA\" in name \\\n",
    "            or \"KATAKANA\" in name:\n",
    "                return True\n",
    "        except:\n",
    "          continue\n",
    "    return False\n",
    "\n",
    "stopwords = {x: 1 for x in stopwords.words('english')}\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "df['new_tags'] = df['tags'].astype(str).apply(lambda x: x.replace('|',' '))\n",
    "df['all_text'] =  (df['channelTitle'].fillna('') + ' ' + df['description'].fillna('') + ' ' + df['title'].fillna('')+ ' ' + df['new_tags'].fillna('')).astype(str)\n",
    "df['all_text'] = preprocess(df['all_text'])\n",
    "text_cols = ['channelTitle','description','title','new_tags','all_text']\n",
    "for cols in text_cols:   \n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols + '_num_cap'] = df[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))\n",
    "    df[cols + '_num_low'] = df[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    df[cols + '_num_dig'] = df[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    df[cols + '_num_engdig'] = df[cols].apply(lambda x: count_regexp_occ('[A-Za-z0-9]', x))    \n",
    "    df[cols + '_isja'] = df[cols].apply(lambda x: 1 if is_japanese(x) else 0)\n",
    "    df[cols + '_isalpha'] = df[cols].apply(lambda x: 1 if x.encode('utf-8').isalnum() else 0)\n",
    "    \n",
    "    df[cols + '_num_pun'] = df[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    df[cols + '_num_space'] = df[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    \n",
    "    df[cols + '_ratio_unique_words'] = df[cols+'_num_unique_words'] / (df[cols+'_num_words']+1) # Count Unique Words    \n",
    "\n",
    "    df[cols +'_num_stopwords'] = df[cols].apply(lambda x: len([w for w in x.split() if w in stopwords]))\n",
    "    df[cols +'_num_words_upper'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    df[cols +'_num_words_lower'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    df[cols +'_num_words_title'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    df[cols +'_music'] = df[cols].apply(lambda x: 1 if 'music' in x.lower() else 0)\n",
    "    df[cols +'_official'] = df[cols].apply(lambda x: 1 if 'official' in x.lower() else 0)\n",
    "    df[cols +'_ja_official'] = df[cols].apply(lambda x: 1 if '公式' in x else 0) \n",
    "    df[cols +'_cm'] = df[cols].apply(lambda x: 1 if 'cm' in x.lower() else 0)     \n",
    "    df[cols +'_http'] = df[cols].apply(lambda x: 1 if 'http' in x.lower() else 0)    \n",
    "    df[cols +'_movie'] = df[cols].apply(lambda x: 1 if 'movie' in x.lower() else 0)    \n",
    "    df[cols +'_jp'] = df[cols].apply(lambda x: 1 if 'jp' in x.lower() else 0)     \n",
    "    df[cols +'_youtube'] = df[cols].apply(lambda x: 1 if 'youtube' in x.lower() else 0)         \n",
    "    df[cols +'_jp_movie'] = df[cols].apply(lambda x: 1 if '映画' in x else 0)      \n",
    "    df[cols +'_jp_director'] = df[cols].apply(lambda x: 1 if '監督' in x else 0)       \n",
    "    df[cols +'_jp_tohaku'] = df[cols].apply(lambda x: 1 if '東宝' in x else 0)\n",
    "    \n",
    "text_cols = ['description']\n",
    "for cols in text_cols:       \n",
    "    df[cols + '_url_num'] = df[cols].apply(lambda x: get_url_num(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channelTitle\n",
      "CPU times: user 1.42 s, sys: 81.1 ms, total: 1.5 s\n",
      "Wall time: 1.38 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.74256423e-18</td>\n",
       "      <td>-3.81323150e-18</td>\n",
       "      <td>-1.25109462e-18</td>\n",
       "      <td>4.44693285e-19</td>\n",
       "      <td>9.95385054e-19</td>\n",
       "      <td>-1.53437903e-18</td>\n",
       "      <td>1.71560878e-18</td>\n",
       "      <td>-4.40172695e-18</td>\n",
       "      <td>-1.71971734e-18</td>\n",
       "      <td>-1.43261073e-19</td>\n",
       "      <td>7.82597401e-19</td>\n",
       "      <td>2.00845516e-18</td>\n",
       "      <td>-1.07859901e-18</td>\n",
       "      <td>-1.56785881e-18</td>\n",
       "      <td>6.39552354e-20</td>\n",
       "      <td>-9.96725733e-19</td>\n",
       "      <td>-3.63714390e-18</td>\n",
       "      <td>1.34148475e-18</td>\n",
       "      <td>1.80345491e-18</td>\n",
       "      <td>-2.82117808e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.23333834e-09</td>\n",
       "      <td>4.28460738e-03</td>\n",
       "      <td>2.76656988e-01</td>\n",
       "      <td>1.37113123e-15</td>\n",
       "      <td>-4.80227371e-04</td>\n",
       "      <td>7.73523344e-07</td>\n",
       "      <td>-3.30685109e-06</td>\n",
       "      <td>2.13359328e-16</td>\n",
       "      <td>6.55812106e-07</td>\n",
       "      <td>-6.27053210e-04</td>\n",
       "      <td>-4.07545773e-16</td>\n",
       "      <td>-2.36424972e-05</td>\n",
       "      <td>1.36989828e-05</td>\n",
       "      <td>2.78542857e-07</td>\n",
       "      <td>-3.35686045e-16</td>\n",
       "      <td>2.47592329e-04</td>\n",
       "      <td>-5.62360426e-04</td>\n",
       "      <td>2.76145171e-16</td>\n",
       "      <td>-6.58754482e-17</td>\n",
       "      <td>6.42442131e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.37528385e-18</td>\n",
       "      <td>-5.16734484e-18</td>\n",
       "      <td>-1.68296775e-18</td>\n",
       "      <td>6.56619586e-19</td>\n",
       "      <td>1.47375708e-18</td>\n",
       "      <td>-2.26345088e-18</td>\n",
       "      <td>2.65552213e-18</td>\n",
       "      <td>-6.06332617e-18</td>\n",
       "      <td>-2.53827966e-18</td>\n",
       "      <td>-3.24431500e-19</td>\n",
       "      <td>1.01491360e-18</td>\n",
       "      <td>2.71972008e-18</td>\n",
       "      <td>-1.32898061e-18</td>\n",
       "      <td>-2.12493516e-18</td>\n",
       "      <td>3.38558537e-19</td>\n",
       "      <td>-1.12147056e-18</td>\n",
       "      <td>-4.80240011e-18</td>\n",
       "      <td>1.74150189e-18</td>\n",
       "      <td>2.48161495e-18</td>\n",
       "      <td>-3.56691647e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.00887545e-17</td>\n",
       "      <td>-4.02019822e-17</td>\n",
       "      <td>3.10331706e-18</td>\n",
       "      <td>-5.75261794e-19</td>\n",
       "      <td>4.76878313e-18</td>\n",
       "      <td>-4.48477603e-18</td>\n",
       "      <td>-5.22152463e-18</td>\n",
       "      <td>-6.07871035e-18</td>\n",
       "      <td>1.21758435e-17</td>\n",
       "      <td>1.61752162e-17</td>\n",
       "      <td>-1.05375693e-17</td>\n",
       "      <td>-2.61183942e-17</td>\n",
       "      <td>8.35320731e-18</td>\n",
       "      <td>-5.29805501e-18</td>\n",
       "      <td>-1.46742397e-17</td>\n",
       "      <td>-6.64015246e-18</td>\n",
       "      <td>-7.76960693e-18</td>\n",
       "      <td>8.16383457e-18</td>\n",
       "      <td>8.72562863e-18</td>\n",
       "      <td>-2.77226427e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.61881162e-18</td>\n",
       "      <td>3.02018759e-18</td>\n",
       "      <td>-2.64154474e-18</td>\n",
       "      <td>1.79683778e-18</td>\n",
       "      <td>1.05699816e-18</td>\n",
       "      <td>3.45875622e-19</td>\n",
       "      <td>3.65413530e-18</td>\n",
       "      <td>-5.00577926e-18</td>\n",
       "      <td>-1.38234199e-18</td>\n",
       "      <td>-2.39957345e-18</td>\n",
       "      <td>5.89817734e-18</td>\n",
       "      <td>-1.06385269e-18</td>\n",
       "      <td>1.08082328e-18</td>\n",
       "      <td>2.71720945e-18</td>\n",
       "      <td>2.47568839e-18</td>\n",
       "      <td>2.94455402e-19</td>\n",
       "      <td>4.67457318e-18</td>\n",
       "      <td>-1.14542771e-18</td>\n",
       "      <td>4.95572657e-19</td>\n",
       "      <td>1.69319771e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49297</th>\n",
       "      <td>6.54758833e-15</td>\n",
       "      <td>2.95936599e-08</td>\n",
       "      <td>4.12667072e-06</td>\n",
       "      <td>-6.06840880e-16</td>\n",
       "      <td>-1.45260832e-08</td>\n",
       "      <td>1.55817180e-10</td>\n",
       "      <td>5.75839262e-10</td>\n",
       "      <td>-4.13102152e-15</td>\n",
       "      <td>9.99993701e-01</td>\n",
       "      <td>2.11800659e-07</td>\n",
       "      <td>-5.44931276e-15</td>\n",
       "      <td>9.44586228e-09</td>\n",
       "      <td>-1.93550131e-07</td>\n",
       "      <td>-3.32938477e-10</td>\n",
       "      <td>-1.93851842e-15</td>\n",
       "      <td>-7.71943524e-08</td>\n",
       "      <td>3.63931473e-09</td>\n",
       "      <td>1.60243071e-16</td>\n",
       "      <td>7.67752970e-16</td>\n",
       "      <td>-6.11370392e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49298</th>\n",
       "      <td>-4.39955024e-19</td>\n",
       "      <td>-9.73885618e-19</td>\n",
       "      <td>-3.21430330e-19</td>\n",
       "      <td>1.01069120e-19</td>\n",
       "      <td>2.64775830e-19</td>\n",
       "      <td>-4.05584066e-19</td>\n",
       "      <td>4.66142111e-19</td>\n",
       "      <td>-1.12813043e-18</td>\n",
       "      <td>-4.64312547e-19</td>\n",
       "      <td>-7.21872464e-20</td>\n",
       "      <td>1.76720913e-19</td>\n",
       "      <td>4.72455786e-19</td>\n",
       "      <td>-2.41569907e-19</td>\n",
       "      <td>-4.09565210e-19</td>\n",
       "      <td>-2.90121582e-20</td>\n",
       "      <td>-1.98380291e-19</td>\n",
       "      <td>-9.36839248e-19</td>\n",
       "      <td>3.24768363e-19</td>\n",
       "      <td>4.50765134e-19</td>\n",
       "      <td>-7.05238647e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49299</th>\n",
       "      <td>5.79496417e-11</td>\n",
       "      <td>5.45360901e-05</td>\n",
       "      <td>6.82882556e-05</td>\n",
       "      <td>3.05796241e-18</td>\n",
       "      <td>-2.97383744e-07</td>\n",
       "      <td>1.30490186e-07</td>\n",
       "      <td>-1.25462320e-08</td>\n",
       "      <td>-2.74854604e-17</td>\n",
       "      <td>2.47868579e-08</td>\n",
       "      <td>3.02920747e-05</td>\n",
       "      <td>2.76603183e-17</td>\n",
       "      <td>2.41863547e-08</td>\n",
       "      <td>1.04371557e-05</td>\n",
       "      <td>-4.98156795e-09</td>\n",
       "      <td>1.01951042e-19</td>\n",
       "      <td>7.48444749e-04</td>\n",
       "      <td>2.99449056e-05</td>\n",
       "      <td>-2.11804882e-18</td>\n",
       "      <td>1.91373222e-18</td>\n",
       "      <td>2.97515574e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49300</th>\n",
       "      <td>9.80235266e-10</td>\n",
       "      <td>1.56082380e-03</td>\n",
       "      <td>8.15688624e-04</td>\n",
       "      <td>1.41238253e-14</td>\n",
       "      <td>6.24838253e-01</td>\n",
       "      <td>-2.26123277e-06</td>\n",
       "      <td>-5.34938493e-07</td>\n",
       "      <td>6.71887527e-15</td>\n",
       "      <td>3.71496433e-09</td>\n",
       "      <td>3.08292864e-05</td>\n",
       "      <td>2.83822772e-14</td>\n",
       "      <td>7.80595711e-01</td>\n",
       "      <td>-1.12468783e-05</td>\n",
       "      <td>-4.07029676e-08</td>\n",
       "      <td>1.81568453e-15</td>\n",
       "      <td>-1.79228812e-06</td>\n",
       "      <td>-2.41043949e-04</td>\n",
       "      <td>-2.45905814e-17</td>\n",
       "      <td>-3.27864508e-17</td>\n",
       "      <td>7.17619984e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49301</th>\n",
       "      <td>4.12311341e-16</td>\n",
       "      <td>8.22855026e-16</td>\n",
       "      <td>-1.04415797e-16</td>\n",
       "      <td>-2.72346025e-17</td>\n",
       "      <td>2.26174670e-16</td>\n",
       "      <td>-1.01360506e-16</td>\n",
       "      <td>-2.47248086e-16</td>\n",
       "      <td>-5.85204952e-18</td>\n",
       "      <td>1.20097647e-16</td>\n",
       "      <td>9.30826325e-17</td>\n",
       "      <td>-4.06932835e-16</td>\n",
       "      <td>-1.96980436e-16</td>\n",
       "      <td>-2.27607105e-16</td>\n",
       "      <td>-8.96287786e-19</td>\n",
       "      <td>1.42685018e-16</td>\n",
       "      <td>-2.36420379e-16</td>\n",
       "      <td>4.95996435e-16</td>\n",
       "      <td>2.91009540e-16</td>\n",
       "      <td>4.03039226e-16</td>\n",
       "      <td>1.47378970e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49302 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0               1               2               3               4               5               6               7               8               9               10              11              12              13              14              15              16              17              18              19\n",
       "0     -1.74256423e-18 -3.81323150e-18 -1.25109462e-18  4.44693285e-19  9.95385054e-19 -1.53437903e-18  1.71560878e-18 -4.40172695e-18 -1.71971734e-18 -1.43261073e-19  7.82597401e-19  2.00845516e-18 -1.07859901e-18 -1.56785881e-18  6.39552354e-20 -9.96725733e-19 -3.63714390e-18  1.34148475e-18  1.80345491e-18 -2.82117808e-19\n",
       "1      3.23333834e-09  4.28460738e-03  2.76656988e-01  1.37113123e-15 -4.80227371e-04  7.73523344e-07 -3.30685109e-06  2.13359328e-16  6.55812106e-07 -6.27053210e-04 -4.07545773e-16 -2.36424972e-05  1.36989828e-05  2.78542857e-07 -3.35686045e-16  2.47592329e-04 -5.62360426e-04  2.76145171e-16 -6.58754482e-17  6.42442131e-11\n",
       "2     -2.37528385e-18 -5.16734484e-18 -1.68296775e-18  6.56619586e-19  1.47375708e-18 -2.26345088e-18  2.65552213e-18 -6.06332617e-18 -2.53827966e-18 -3.24431500e-19  1.01491360e-18  2.71972008e-18 -1.32898061e-18 -2.12493516e-18  3.38558537e-19 -1.12147056e-18 -4.80240011e-18  1.74150189e-18  2.48161495e-18 -3.56691647e-19\n",
       "3     -2.00887545e-17 -4.02019822e-17  3.10331706e-18 -5.75261794e-19  4.76878313e-18 -4.48477603e-18 -5.22152463e-18 -6.07871035e-18  1.21758435e-17  1.61752162e-17 -1.05375693e-17 -2.61183942e-17  8.35320731e-18 -5.29805501e-18 -1.46742397e-17 -6.64015246e-18 -7.76960693e-18  8.16383457e-18  8.72562863e-18 -2.77226427e-18\n",
       "4      1.61881162e-18  3.02018759e-18 -2.64154474e-18  1.79683778e-18  1.05699816e-18  3.45875622e-19  3.65413530e-18 -5.00577926e-18 -1.38234199e-18 -2.39957345e-18  5.89817734e-18 -1.06385269e-18  1.08082328e-18  2.71720945e-18  2.47568839e-18  2.94455402e-19  4.67457318e-18 -1.14542771e-18  4.95572657e-19  1.69319771e-18\n",
       "...               ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...             ...\n",
       "49297  6.54758833e-15  2.95936599e-08  4.12667072e-06 -6.06840880e-16 -1.45260832e-08  1.55817180e-10  5.75839262e-10 -4.13102152e-15  9.99993701e-01  2.11800659e-07 -5.44931276e-15  9.44586228e-09 -1.93550131e-07 -3.32938477e-10 -1.93851842e-15 -7.71943524e-08  3.63931473e-09  1.60243071e-16  7.67752970e-16 -6.11370392e-14\n",
       "49298 -4.39955024e-19 -9.73885618e-19 -3.21430330e-19  1.01069120e-19  2.64775830e-19 -4.05584066e-19  4.66142111e-19 -1.12813043e-18 -4.64312547e-19 -7.21872464e-20  1.76720913e-19  4.72455786e-19 -2.41569907e-19 -4.09565210e-19 -2.90121582e-20 -1.98380291e-19 -9.36839248e-19  3.24768363e-19  4.50765134e-19 -7.05238647e-20\n",
       "49299  5.79496417e-11  5.45360901e-05  6.82882556e-05  3.05796241e-18 -2.97383744e-07  1.30490186e-07 -1.25462320e-08 -2.74854604e-17  2.47868579e-08  3.02920747e-05  2.76603183e-17  2.41863547e-08  1.04371557e-05 -4.98156795e-09  1.01951042e-19  7.48444749e-04  2.99449056e-05 -2.11804882e-18  1.91373222e-18  2.97515574e-14\n",
       "49300  9.80235266e-10  1.56082380e-03  8.15688624e-04  1.41238253e-14  6.24838253e-01 -2.26123277e-06 -5.34938493e-07  6.71887527e-15  3.71496433e-09  3.08292864e-05  2.83822772e-14  7.80595711e-01 -1.12468783e-05 -4.07029676e-08  1.81568453e-15 -1.79228812e-06 -2.41043949e-04 -2.45905814e-17 -3.27864508e-17  7.17619984e-14\n",
       "49301  4.12311341e-16  8.22855026e-16 -1.04415797e-16 -2.72346025e-17  2.26174670e-16 -1.01360506e-16 -2.47248086e-16 -5.85204952e-18  1.20097647e-16  9.30826325e-17 -4.06932835e-16 -1.96980436e-16 -2.27607105e-16 -8.96287786e-19  1.42685018e-16 -2.36420379e-16  4.95996435e-16  2.91009540e-16  4.03039226e-16  1.47378970e-16\n",
       "\n",
       "[49302 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### TFIDF Vectorizer ###\n",
    "### SVD Components ###\n",
    "n_comp = 20\n",
    "\n",
    "# for i in ['channelTitle','description','title','all_text']:#,'new_title','new_description',\n",
    "for i in ['channelTitle']:#,'new_title','new_description',\n",
    "    print (i)\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,2))\n",
    "    text_tfidf = tfidf_vec.fit_transform(df[i].values.tolist() )\n",
    "#     d = pd.DataFrame(text_tfidf.toarray(), columns=tfidf_vec.get_feature_names())\n",
    "    text_svd = TruncatedSVD(n_components=n_comp, algorithm='arpack',random_state=9999)\n",
    "    df_svd = pd.DataFrame(text_svd.fit_transform(text_tfidf))\n",
    "#     df_svd.columns = ['svd_'+str(i)+str(j+1) for j in range(n_comp)]\n",
    "#     df = pd.concat([df,df_svd],axis=1)\n",
    "    \n",
    "# for i in ['new_tags',]:\n",
    "#     print (i)\n",
    "#     tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "#     text_tfidf = tfidf_vec.fit_transform(df[i].values.tolist() )\n",
    "#     text_svd = TruncatedSVD(n_components=n_comp, algorithm='arpack',random_state=9999)\n",
    "#     df_svd = pd.DataFrame(text_svd.fit_transform(text_tfidf))\n",
    "#     df_svd.columns = ['svd_char_'+str(i)+str(j+1) for j in range(n_comp)]\n",
    "#     df = pd.concat([df,df_svd],axis=1)\n",
    "df_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "for i in ['title']:#,'new_title','new_description',\n",
    "    print (i)\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,2))\n",
    "    text_tfidf = tfidf_vec.fit_transform(df[i].values.tolist() )\n",
    "    d = pd.DataFrame(text_tfidf.toarray(), columns=tfidf_vec.get_feature_names())\n",
    "    \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:33<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 s, sys: 386 ms, total: 33.3 s\n",
      "Wall time: 33.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "def agg(df,agg_cols):\n",
    "    for c in tqdm(agg_cols):\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(c['groupby']), c['agg'], c['target'])\n",
    "        if c['agg'] == 'diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.diff(c['para1']).shift(c['para2']))\n",
    "        elif c['agg'] == 'lag':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].shift(c['para1'])\n",
    "        elif c['agg'] == 'rolling_sum':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).sum().shift(c['para2']))                      \n",
    "        elif c['agg'] == 'rolling_mean':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).mean().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_max':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).max().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_min':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).min().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_median':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).median().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_std':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).std().shift(c['para2']))  \n",
    "        elif c['agg'] == 'cumcount':\n",
    "            df[new_feature] = df.groupby(c['groupby']).cumcount()   \n",
    "        elif c['agg'] == 'cumsum':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cumsum())             \n",
    "        elif c['agg'] == 'cummax':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cummax()) \n",
    "        elif c['agg'] == 'cummin':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cummin()) \n",
    "        elif c['agg'] == 'cummean':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cumsum()) / (df.groupby(c['groupby']).cumcount() + 1)\n",
    "        elif c['agg'] == 'mean_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('mean') - df[c['target']]\n",
    "        elif c['agg'] == 'mean_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('mean') / (1+df[c['target']])\n",
    "        elif c['agg'] == 'trim_mean':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: stats.trim_mean(x, 0.1))             \n",
    "        elif c['agg'] == 'trim_mean_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: stats.trim_mean(x, 0.1)) - df[c['target']]\n",
    "        elif c['agg'] == 'max_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') - df[c['target']]\n",
    "        elif c['agg'] == 'max_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') / (1+df[c['target']])   \n",
    "        elif c['agg'] == 'min_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('min')- df[c['target']]\n",
    "        elif c['agg'] == 'min_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('min')/ (1+df[c['target']])    \n",
    "        elif c['agg'] == 'max_min_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') - df.groupby(c['groupby'])[c['target']].transform('min')\n",
    "        elif c['agg'] == 'max_min_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') / (1+df.groupby(c['groupby'])[c['target']].transform('min'))             \n",
    "        elif c['agg'] == 'median_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('median') - df[c['target']]\n",
    "        elif c['agg'] == 'median_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('median') / (1+df[c['target']])    \n",
    "        elif c['agg'] == 'mode':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].apply(pd.Series.mode).reset_index(drop=True)            \n",
    "        else:    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(c['agg'])\n",
    "\n",
    "agg_cols = [\n",
    "\n",
    "# ############################ aggregation##################################\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'count'},\n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'count'},\n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'trim_mean'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'median'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'mean_ratio'},  \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'median_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'median_ratio'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'max_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'max_ratio'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'min_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'min_ratio'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'trim_mean_diff'}, \n",
    "\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'trim_mean'},    \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'median'},    \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'mean_ratio'},  \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'median_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'median_ratio'},     \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'max_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'max_ratio'},\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'min_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'min_ratio'},\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'trim_mean_diff'}, \n",
    "    \n",
    "  \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'trim_mean'},    \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'median'},    \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'mean_ratio'},  \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'median_diff'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'median_ratio'},     \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'max_diff'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'max_ratio'},\n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'min_diff'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'min_ratio'},\n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'trim_mean_diff'}, \n",
    "    \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'trim_mean'},    \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'median'},    \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'mean_ratio'},  \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'median_diff'}, \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'median_ratio'},     \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'max_diff'}, \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'max_ratio'},\n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'min_diff'}, \n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'min_ratio'},\n",
    "    {'groupby': ['tags_encoder'], 'target':'comment_count', 'agg':'trim_mean_diff'}, \n",
    "       \n",
    "    {'groupby': ['comments_disabled'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['comments_disabled'], 'target':'comment_count', 'agg':'mean_ratio'},     \n",
    "    \n",
    "     \n",
    "]\n",
    "\n",
    "agg(df,agg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical features: 329 ['categoryId', 'comment_count', 'channelId_encoder', 'channelTitle_encoder', 'collection_date_encoder', 'description_encoder', 'tags_encoder', 'comments_disabled_encoder', 'c_year', 'c_month', 'c_day', 'c_dayofweek', 'year', 'month', 'weekofyear', 'day', 'dayofweek', 'hour', 'minute', 'seconds_from_publish', 'days_from_publish', 'months_from_publish', 'years_from_publish', 'days_from_publish_start', 'days_from_cdate_start', 'channelTitle_num_cap', 'channelTitle_num_low', 'channelTitle_num_dig', 'channelTitle_num_engdig', 'channelTitle_isja', 'channelTitle_isalpha', 'channelTitle_num_pun', 'channelTitle_num_space', 'channelTitle_num_chars', 'channelTitle_num_words', 'channelTitle_num_unique_words', 'channelTitle_ratio_unique_words', 'channelTitle_num_stopwords', 'channelTitle_num_words_upper', 'channelTitle_num_words_lower', 'channelTitle_num_words_title', 'channelTitle_music', 'channelTitle_official', 'channelTitle_ja_official', 'channelTitle_cm', 'channelTitle_http', 'channelTitle_movie', 'channelTitle_jp', 'channelTitle_youtube', 'channelTitle_jp_movie', 'channelTitle_jp_director', 'channelTitle_jp_tohaku', 'description_num_cap', 'description_num_low', 'description_num_dig', 'description_num_engdig', 'description_isja', 'description_isalpha', 'description_num_pun', 'description_num_space', 'description_num_chars', 'description_num_words', 'description_num_unique_words', 'description_ratio_unique_words', 'description_num_stopwords', 'description_num_words_upper', 'description_num_words_lower', 'description_num_words_title', 'description_music', 'description_official', 'description_ja_official', 'description_cm', 'description_http', 'description_movie', 'description_jp', 'description_youtube', 'description_jp_movie', 'description_jp_director', 'description_jp_tohaku', 'title_num_cap', 'title_num_low', 'title_num_dig', 'title_num_engdig', 'title_isja', 'title_isalpha', 'title_num_pun', 'title_num_space', 'title_num_chars', 'title_num_words', 'title_num_unique_words', 'title_ratio_unique_words', 'title_num_stopwords', 'title_num_words_upper', 'title_num_words_lower', 'title_num_words_title', 'title_music', 'title_official', 'title_ja_official', 'title_cm', 'title_http', 'title_movie', 'title_jp', 'title_youtube', 'title_jp_movie', 'title_jp_director', 'title_jp_tohaku', 'new_tags_num_cap', 'new_tags_num_low', 'new_tags_num_dig', 'new_tags_num_engdig', 'new_tags_isja', 'new_tags_isalpha', 'new_tags_num_pun', 'new_tags_num_space', 'new_tags_num_chars', 'new_tags_num_words', 'new_tags_num_unique_words', 'new_tags_ratio_unique_words', 'new_tags_num_stopwords', 'new_tags_num_words_upper', 'new_tags_num_words_lower', 'new_tags_num_words_title', 'new_tags_music', 'new_tags_official', 'new_tags_ja_official', 'new_tags_cm', 'new_tags_http', 'new_tags_movie', 'new_tags_jp', 'new_tags_youtube', 'new_tags_jp_movie', 'new_tags_jp_director', 'new_tags_jp_tohaku', 'all_text_num_cap', 'all_text_num_low', 'all_text_num_dig', 'all_text_num_engdig', 'all_text_isja', 'all_text_isalpha', 'all_text_num_pun', 'all_text_num_space', 'all_text_num_chars', 'all_text_num_words', 'all_text_num_unique_words', 'all_text_ratio_unique_words', 'all_text_num_stopwords', 'all_text_num_words_upper', 'all_text_num_words_lower', 'all_text_num_words_title', 'all_text_music', 'all_text_official', 'all_text_ja_official', 'all_text_cm', 'all_text_http', 'all_text_movie', 'all_text_jp', 'all_text_youtube', 'all_text_jp_movie', 'all_text_jp_director', 'all_text_jp_tohaku', 'description_url_num', 'svd_channelTitle1', 'svd_channelTitle2', 'svd_channelTitle3', 'svd_channelTitle4', 'svd_channelTitle5', 'svd_channelTitle6', 'svd_channelTitle7', 'svd_channelTitle8', 'svd_channelTitle9', 'svd_channelTitle10', 'svd_channelTitle11', 'svd_channelTitle12', 'svd_channelTitle13', 'svd_channelTitle14', 'svd_channelTitle15', 'svd_channelTitle16', 'svd_channelTitle17', 'svd_channelTitle18', 'svd_channelTitle19', 'svd_channelTitle20', 'svd_description1', 'svd_description2', 'svd_description3', 'svd_description4', 'svd_description5', 'svd_description6', 'svd_description7', 'svd_description8', 'svd_description9', 'svd_description10', 'svd_description11', 'svd_description12', 'svd_description13', 'svd_description14', 'svd_description15', 'svd_description16', 'svd_description17', 'svd_description18', 'svd_description19', 'svd_description20', 'svd_title1', 'svd_title2', 'svd_title3', 'svd_title4', 'svd_title5', 'svd_title6', 'svd_title7', 'svd_title8', 'svd_title9', 'svd_title10', 'svd_title11', 'svd_title12', 'svd_title13', 'svd_title14', 'svd_title15', 'svd_title16', 'svd_title17', 'svd_title18', 'svd_title19', 'svd_title20', 'svd_all_text1', 'svd_all_text2', 'svd_all_text3', 'svd_all_text4', 'svd_all_text5', 'svd_all_text6', 'svd_all_text7', 'svd_all_text8', 'svd_all_text9', 'svd_all_text10', 'svd_all_text11', 'svd_all_text12', 'svd_all_text13', 'svd_all_text14', 'svd_all_text15', 'svd_all_text16', 'svd_all_text17', 'svd_all_text18', 'svd_all_text19', 'svd_all_text20', 'svd_char_new_tags1', 'svd_char_new_tags2', 'svd_char_new_tags3', 'svd_char_new_tags4', 'svd_char_new_tags5', 'svd_char_new_tags6', 'svd_char_new_tags7', 'svd_char_new_tags8', 'svd_char_new_tags9', 'svd_char_new_tags10', 'svd_char_new_tags11', 'svd_char_new_tags12', 'svd_char_new_tags13', 'svd_char_new_tags14', 'svd_char_new_tags15', 'svd_char_new_tags16', 'svd_char_new_tags17', 'svd_char_new_tags18', 'svd_char_new_tags19', 'svd_char_new_tags20', 'channelTitle_encoder_count_likes', 'categoryId_count_likes', 'channelTitle_encoder_sum_comment_count', 'channelTitle_encoder_mean_comment_count', 'channelTitle_encoder_trim_mean_comment_count', 'channelTitle_encoder_median_comment_count', 'channelTitle_encoder_max_comment_count', 'channelTitle_encoder_min_comment_count', 'channelTitle_encoder_std_comment_count', 'channelTitle_encoder_mean_diff_comment_count', 'channelTitle_encoder_mean_ratio_comment_count', 'channelTitle_encoder_median_diff_comment_count', 'channelTitle_encoder_median_ratio_comment_count', 'channelTitle_encoder_max_diff_comment_count', 'channelTitle_encoder_max_ratio_comment_count', 'channelTitle_encoder_min_diff_comment_count', 'channelTitle_encoder_min_ratio_comment_count', 'channelTitle_encoder_trim_mean_diff_comment_count', 'categoryId_sum_comment_count', 'categoryId_mean_comment_count', 'categoryId_trim_mean_comment_count', 'categoryId_median_comment_count', 'categoryId_max_comment_count', 'categoryId_min_comment_count', 'categoryId_std_comment_count', 'categoryId_mean_diff_comment_count', 'categoryId_mean_ratio_comment_count', 'categoryId_median_diff_comment_count', 'categoryId_median_ratio_comment_count', 'categoryId_max_diff_comment_count', 'categoryId_max_ratio_comment_count', 'categoryId_min_diff_comment_count', 'categoryId_min_ratio_comment_count', 'categoryId_trim_mean_diff_comment_count', 'year_sum_comment_count', 'year_mean_comment_count', 'year_trim_mean_comment_count', 'year_median_comment_count', 'year_max_comment_count', 'year_min_comment_count', 'year_std_comment_count', 'year_mean_diff_comment_count', 'year_mean_ratio_comment_count', 'year_median_diff_comment_count', 'year_median_ratio_comment_count', 'year_max_diff_comment_count', 'year_max_ratio_comment_count', 'year_min_diff_comment_count', 'year_min_ratio_comment_count', 'year_trim_mean_diff_comment_count', 'tags_encoder_sum_comment_count', 'tags_encoder_mean_comment_count', 'tags_encoder_trim_mean_comment_count', 'tags_encoder_median_comment_count', 'tags_encoder_max_comment_count', 'tags_encoder_min_comment_count', 'tags_encoder_std_comment_count', 'tags_encoder_mean_diff_comment_count', 'tags_encoder_mean_ratio_comment_count', 'tags_encoder_median_diff_comment_count', 'tags_encoder_median_ratio_comment_count', 'tags_encoder_max_diff_comment_count', 'tags_encoder_max_ratio_comment_count', 'tags_encoder_min_diff_comment_count', 'tags_encoder_min_ratio_comment_count', 'tags_encoder_trim_mean_diff_comment_count', 'comments_disabled_mean_diff_comment_count', 'comments_disabled_mean_ratio_comment_count']\n",
      "FOLD:0\n",
      "train_x shape: (36800, 329) 6.765581345409805\n",
      "valid_x shape: (9200, 329) 6.75487155501896\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's rmse: 0.483166\tvalid_0's rmse: 0.827319\n",
      "[1000]\ttraining's rmse: 0.328671\tvalid_0's rmse: 0.818515\n",
      "[1500]\ttraining's rmse: 0.233824\tvalid_0's rmse: 0.816688\n",
      "[2000]\ttraining's rmse: 0.170839\tvalid_0's rmse: 0.81623\n",
      "[2500]\ttraining's rmse: 0.126783\tvalid_0's rmse: 0.81597\n",
      "Early stopping, best iteration is:\n",
      "[2322]\ttraining's rmse: 0.141046\tvalid_0's rmse: 0.815686\n",
      "('comment_count', 228356.39536882192)\n",
      "('year_min_diff_comment_count', 206414.2894514352)\n",
      "('comments_disabled_mean_diff_comment_count', 180415.2776164189)\n",
      "('categoryId_min_diff_comment_count', 141767.61215931177)\n",
      "('year_max_ratio_comment_count', 55011.309470541775)\n",
      "('comments_disabled_mean_ratio_comment_count', 38475.20053194463)\n",
      "('days_from_publish', 9043.082282811403)\n",
      "('days_from_publish_start', 7513.840056180954)\n",
      "('months_from_publish', 7487.246441066265)\n",
      "('channelTitle_encoder_mean_comment_count', 7235.640158563852)\n",
      "('year_mean_ratio_comment_count', 7078.616304934025)\n",
      "('channelTitle_encoder_count_likes', 5126.257529549301)\n",
      "('channelTitle_encoder_median_comment_count', 4462.327462546527)\n",
      "('categoryId_count_likes', 4335.736421316862)\n",
      "('channelTitle_encoder_max_comment_count', 4309.710576057434)\n",
      "('svd_all_text5', 3607.1364919915795)\n",
      "('svd_all_text7', 3328.423536889255)\n",
      "('svd_title14', 3073.8867229931056)\n",
      "('svd_all_text3', 3050.2190681248903)\n",
      "('channelTitle_encoder_trim_mean_comment_count', 3024.1918978840113)\n",
      "('all_text_isja', 2973.974622488022)\n",
      "('categoryId_mean_ratio_comment_count', 2754.94336450845)\n",
      "('svd_char_new_tags3', 2590.891282275319)\n",
      "('channelTitle_encoder_sum_comment_count', 2489.2587124183774)\n",
      "('year_trim_mean_diff_comment_count', 2308.3412188217044)\n",
      "('description_num_pun', 2265.1889034360647)\n",
      "('svd_channelTitle6', 2237.1364932134748)\n",
      "('year_median_diff_comment_count', 2152.6532865017653)\n",
      "('description_youtube', 2126.61036247015)\n",
      "('year_median_ratio_comment_count', 2105.100090738386)\n",
      "[0.8156862584628051]\n",
      "FOLD:1\n",
      "train_x shape: (36800, 329) 6.760639708115003\n",
      "valid_x shape: (9200, 329) 6.774638104198166\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's rmse: 0.484064\tvalid_0's rmse: 0.823112\n",
      "[1000]\ttraining's rmse: 0.32909\tvalid_0's rmse: 0.818075\n",
      "[1500]\ttraining's rmse: 0.234424\tvalid_0's rmse: 0.816069\n",
      "[2000]\ttraining's rmse: 0.170904\tvalid_0's rmse: 0.815695\n",
      "Early stopping, best iteration is:\n",
      "[1645]\ttraining's rmse: 0.21346\tvalid_0's rmse: 0.815384\n",
      "('comment_count', 245664.89729094505)\n",
      "('year_min_diff_comment_count', 190453.52914822102)\n",
      "('categoryId_min_diff_comment_count', 163933.68540215492)\n",
      "('comments_disabled_mean_diff_comment_count', 141561.90247294307)\n",
      "('comments_disabled_mean_ratio_comment_count', 56066.546573892236)\n",
      "('year_max_ratio_comment_count', 48423.85187621415)\n",
      "('days_from_publish', 12017.77871723473)\n",
      "('days_from_publish_start', 8680.409487843513)\n",
      "('channelTitle_encoder_mean_comment_count', 6600.962751090527)\n",
      "('year_mean_ratio_comment_count', 5863.131617486477)\n",
      "('svd_title14', 5852.901396244764)\n",
      "('channelTitle_encoder_median_comment_count', 5824.857412710786)\n",
      "('year_median_diff_comment_count', 5343.146151468158)\n",
      "('months_from_publish', 5083.757305204868)\n",
      "('svd_all_text5', 4710.524851977825)\n",
      "('channelTitle_encoder_max_comment_count', 4536.343293219805)\n",
      "('channelTitle_encoder_trim_mean_comment_count', 3878.4848530590534)\n",
      "('channelTitle_encoder_count_likes', 3703.191032141447)\n",
      "('categoryId_count_likes', 3451.7003299593925)\n",
      "('all_text_isja', 3253.0178686380386)\n",
      "('categoryId_mean_ratio_comment_count', 3151.5739099681377)\n",
      "('year_median_ratio_comment_count', 3138.9656637609005)\n",
      "('svd_all_text7', 3036.3345103412867)\n",
      "('description_url_num', 2749.833392113447)\n",
      "('channelTitle_encoder_median_ratio_comment_count', 2743.991089820862)\n",
      "('svd_char_new_tags3', 2565.247299015522)\n",
      "('categoryId_mean_diff_comment_count', 2493.274229258299)\n",
      "('categoryId_median_comment_count', 2233.684930473566)\n",
      "('svd_channelTitle6', 2221.3507267981768)\n",
      "('year_trim_mean_diff_comment_count', 2199.2675954550505)\n",
      "[0.8156862584628051, 0.8153843269449622]\n",
      "FOLD:2\n",
      "train_x shape: (36800, 329) 6.7700786911156285\n",
      "valid_x shape: (9200, 329) 6.736882172195667\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's rmse: 0.484107\tvalid_0's rmse: 0.818226\n",
      "[1000]\ttraining's rmse: 0.329169\tvalid_0's rmse: 0.811696\n",
      "[1500]\ttraining's rmse: 0.233468\tvalid_0's rmse: 0.809671\n",
      "[2000]\ttraining's rmse: 0.171347\tvalid_0's rmse: 0.809391\n",
      "[2500]\ttraining's rmse: 0.127597\tvalid_0's rmse: 0.809468\n",
      "Early stopping, best iteration is:\n",
      "[2322]\ttraining's rmse: 0.141405\tvalid_0's rmse: 0.809069\n",
      "('comment_count', 251968.5145135224)\n",
      "('categoryId_min_diff_comment_count', 175036.39057588577)\n",
      "('comments_disabled_mean_diff_comment_count', 152441.0478850603)\n",
      "('year_min_diff_comment_count', 149949.93590319157)\n",
      "('year_max_ratio_comment_count', 58170.37449707836)\n",
      "('comments_disabled_mean_ratio_comment_count', 48131.77894548327)\n",
      "('days_from_publish_start', 8344.032327838242)\n",
      "('days_from_publish', 8199.431529685855)\n",
      "('channelTitle_encoder_mean_comment_count', 6682.109762072563)\n",
      "('months_from_publish', 6201.776527792215)\n",
      "('year_mean_ratio_comment_count', 6080.1565317213535)\n",
      "('channelTitle_encoder_trim_mean_comment_count', 4798.340233594179)\n",
      "('svd_all_text5', 4626.937981136143)\n",
      "('channelTitle_encoder_max_comment_count', 4473.342354379594)\n",
      "('categoryId_count_likes', 4251.322630599141)\n",
      "('channelTitle_encoder_median_comment_count', 4145.790628157556)\n",
      "('channelTitle_encoder_count_likes', 4056.9555986747146)\n",
      "('year_median_diff_comment_count', 3523.91062438488)\n",
      "('svd_title14', 3415.1471170932055)\n",
      "('svd_all_text7', 3108.0737650468946)\n",
      "('all_text_isja', 2843.3789543807507)\n",
      "('categoryId_mean_diff_comment_count', 2682.3060354217887)\n",
      "('days_from_cdate_start', 2540.1227716356516)\n",
      "('categoryId_median_comment_count', 2456.8508718311787)\n",
      "('comments_disabled_encoder', 2448.2113801836967)\n",
      "('svd_char_new_tags3', 2437.029489982873)\n",
      "('svd_all_text13', 2387.9654689878225)\n",
      "('year_trim_mean_diff_comment_count', 2325.5124387294054)\n",
      "('channelTitle_encoder_mean_ratio_comment_count', 2161.9368237778544)\n",
      "('year_median_ratio_comment_count', 2156.80942568928)\n",
      "[0.8156862584628051, 0.8153843269449622, 0.8090686088280225]\n",
      "FOLD:3\n",
      "train_x shape: (36800, 329) 6.759129971469914\n",
      "valid_x shape: (9200, 329) 6.780677050778522\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's rmse: 0.484827\tvalid_0's rmse: 0.81837\n",
      "[1000]\ttraining's rmse: 0.329854\tvalid_0's rmse: 0.81432\n",
      "[1500]\ttraining's rmse: 0.234038\tvalid_0's rmse: 0.812272\n",
      "[2000]\ttraining's rmse: 0.170988\tvalid_0's rmse: 0.811385\n",
      "[2500]\ttraining's rmse: 0.127811\tvalid_0's rmse: 0.810865\n",
      "[3000]\ttraining's rmse: 0.0973077\tvalid_0's rmse: 0.810899\n",
      "Early stopping, best iteration is:\n",
      "[2651]\ttraining's rmse: 0.117254\tvalid_0's rmse: 0.810464\n",
      "('comment_count', 220596.42421638966)\n",
      "('year_min_diff_comment_count', 192124.89186947048)\n",
      "('categoryId_min_diff_comment_count', 186109.96024294943)\n",
      "('comments_disabled_mean_diff_comment_count', 145700.14619097114)\n",
      "('year_max_ratio_comment_count', 61334.61889568716)\n",
      "('comments_disabled_mean_ratio_comment_count', 38817.52583576739)\n",
      "('days_from_publish', 11266.25166125223)\n",
      "('channelTitle_encoder_mean_comment_count', 8507.891835346818)\n",
      "('year_mean_ratio_comment_count', 8052.174214523286)\n",
      "('months_from_publish', 5828.940928801894)\n",
      "('categoryId_count_likes', 4763.826760746539)\n",
      "('svd_title14', 4537.08292619139)\n",
      "('svd_all_text5', 4374.02829124406)\n",
      "('channelTitle_encoder_count_likes', 4347.437800511718)\n",
      "('days_from_publish_start', 4160.858174152672)\n",
      "('description_url_num', 3810.1263312399387)\n",
      "('channelTitle_encoder_median_comment_count', 3799.4398142173886)\n",
      "('days_from_cdate_start', 3722.974817752838)\n",
      "('channelTitle_encoder_trim_mean_comment_count', 3462.8509458191693)\n",
      "('channelTitle_encoder_max_comment_count', 3350.3651832789183)\n",
      "('all_text_isja', 3011.88530574739)\n",
      "('year_median_ratio_comment_count', 2925.4418211318552)\n",
      "('year_trim_mean_diff_comment_count', 2800.12235166505)\n",
      "('svd_all_text7', 2739.2973269037902)\n",
      "('svd_char_new_tags3', 2441.0485648326576)\n",
      "('categoryId', 2424.969665784389)\n",
      "('comments_disabled_encoder', 2301.832031726837)\n",
      "('svd_all_text13', 2248.2150040082633)\n",
      "('categoryId_mean_diff_comment_count', 2220.7988182008266)\n",
      "('new_tags_num_words_lower', 2061.866804923862)\n",
      "[0.8156862584628051, 0.8153843269449622, 0.8090686088280225, 0.8104641970879853]\n",
      "FOLD:4\n",
      "train_x shape: (36800, 329) 6.761767220547829\n",
      "valid_x shape: (9200, 329) 6.770128054466865\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's rmse: 0.487589\tvalid_0's rmse: 0.810287\n",
      "[1000]\ttraining's rmse: 0.331585\tvalid_0's rmse: 0.803529\n",
      "[1500]\ttraining's rmse: 0.236355\tvalid_0's rmse: 0.799263\n",
      "[2000]\ttraining's rmse: 0.171907\tvalid_0's rmse: 0.797715\n",
      "[2500]\ttraining's rmse: 0.127914\tvalid_0's rmse: 0.796342\n",
      "[3000]\ttraining's rmse: 0.0975488\tvalid_0's rmse: 0.79609\n",
      "[3500]\ttraining's rmse: 0.078067\tvalid_0's rmse: 0.795991\n",
      "[4000]\ttraining's rmse: 0.068148\tvalid_0's rmse: 0.7958\n",
      "[4500]\ttraining's rmse: 0.0649755\tvalid_0's rmse: 0.795736\n",
      "[5000]\ttraining's rmse: 0.0634541\tvalid_0's rmse: 0.795758\n",
      "Early stopping, best iteration is:\n",
      "[4773]\ttraining's rmse: 0.0640356\tvalid_0's rmse: 0.795704\n",
      "('comment_count', 235378.47678137012)\n",
      "('year_min_diff_comment_count', 207220.25952771213)\n",
      "('categoryId_min_diff_comment_count', 167118.16289541498)\n",
      "('comments_disabled_mean_diff_comment_count', 139999.99243327044)\n",
      "('year_max_ratio_comment_count', 47062.64204761293)\n",
      "('comments_disabled_mean_ratio_comment_count', 42410.39142323099)\n",
      "('days_from_publish_start', 9186.556945451535)\n",
      "('year_mean_ratio_comment_count', 9094.787898590788)\n",
      "('days_from_publish', 8388.061730164103)\n",
      "('months_from_publish', 7125.479401030578)\n",
      "('channelTitle_encoder_mean_comment_count', 6657.474592234939)\n",
      "('svd_all_text5', 5159.766176957637)\n",
      "('channelTitle_encoder_trim_mean_comment_count', 4825.788978416473)\n",
      "('channelTitle_encoder_count_likes', 4749.7764646187425)\n",
      "('svd_title14', 3996.776722307317)\n",
      "('channelTitle_encoder_max_comment_count', 3995.718235653825)\n",
      "('categoryId_mean_diff_comment_count', 3818.307771633379)\n",
      "('description_url_num', 3052.2544660279527)\n",
      "('categoryId_count_likes', 2868.82967922464)\n",
      "('svd_all_text7', 2794.755695566535)\n",
      "('channelTitle_encoder_median_comment_count', 2636.177251564339)\n",
      "('year_median_ratio_comment_count', 2604.729246637784)\n",
      "('channelTitle_encoder_sum_comment_count', 2583.0409773457795)\n",
      "('svd_char_new_tags3', 2551.4740867065266)\n",
      "('year_trim_mean_diff_comment_count', 2517.8135408489034)\n",
      "('categoryId_median_comment_count', 2356.7303986605257)\n",
      "('categoryId_mean_ratio_comment_count', 2338.9055728428066)\n",
      "('new_tags_num_words_lower', 2289.6060670102015)\n",
      "('new_tags_num_space', 2256.7426296221092)\n",
      "('all_text_isja', 2236.5878906548023)\n",
      "[0.8156862584628051, 0.8153843269449622, 0.8090686088280225, 0.8104641970879853, 0.7957159435999459]\n",
      "Full OOF RMSE 0.809296\n",
      "CPU times: user 27min 48s, sys: 19.8 s, total: 28min 8s\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import catboost as cat\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import svm, neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "def preprocess(train_df,test_df,feats):\n",
    "    train_df = train_df.replace([np.inf, -np.inf], np.nan)\n",
    "    train_df = train_df.fillna(0) \n",
    "\n",
    "    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n",
    "    test_df = test_df.fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_df[feats] = scaler.fit_transform(train_df[feats])\n",
    "    test_df[feats] = scaler.transform(test_df[feats])\n",
    "    \n",
    "    return train_df[feats], test_df[feats]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))** .5\n",
    "\n",
    "def target_encoder_kfold(train_df,test_df,col,target,method):\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_df['y_bin'])):\n",
    "        print ('FOLD:' + str(n_fold))\n",
    "        train_x = train_df.iloc[train_idx]\n",
    "        valid_x = train_df.iloc[valid_idx] \n",
    "        if method == 'mean':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].apply(lambda x: stats.trim_mean(x, 0.01)))\n",
    "        if method == 'median':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].median())   \n",
    "        if method == 'max':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].median())  \n",
    "        if method == 'min':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].median())              \n",
    "    if method == 'mean':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].apply(lambda x: stats.trim_mean(x, 0.01)))\n",
    "    if method == 'median':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].median())\n",
    "    if method == 'max':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].max())\n",
    "    if method == 'min':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].min())        \n",
    "    return oof_preds,sub_preds\n",
    "\n",
    "def lgb_kfold(train_df,test_df,features,target,cat_features,folds,params,use_pseudo=False,sampling=False):\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    cv_list = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[features], train_df[features])):\n",
    "        print ('FOLD:' + str(n_fold))\n",
    "        \n",
    "        train_x, train_y = train_df[features].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "        \n",
    "        # remove outliers\n",
    "        if sampling is True:\n",
    "            valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx] \n",
    "            train_df_new = train_df.iloc[train_idx]\n",
    "            train_df_new = train_df_new[train_df_new['y_residual']<3.2]#3\n",
    "            train_x, train_y = train_df_new[features], train_df_new[target]\n",
    "            \n",
    "        # target encoding\n",
    "#         df_train = train_df.iloc[train_idx]\n",
    "#         train_df_new,test_df_new = train_df,test_df \n",
    "#         cat_features=['age' ,'education' ,'num_child' ,'partner' ,'position' ,'service_length' ,'sex']\n",
    "#         for col in tqdm(cat_features):\n",
    "#             train_df_new,test_df_new = target_mean(df_train,train_df_new,test_df_new,col,target)\n",
    "      \n",
    "#         print (train_df_new.columns.values)\n",
    "#         features = [f for f in train_df_new.columns if f not in drop_features]\n",
    "#         train_x, train_y = train_df_new[features].iloc[train_idx], train_df_new[target].iloc[train_idx]\n",
    "#         valid_x, valid_y = train_df_new[features].iloc[valid_idx], train_df_new[target].iloc[valid_idx] \n",
    "#         test_df = test_df_new.copy()\n",
    "        \n",
    "        \n",
    "       # pseudo \n",
    "        if use_pseudo is True:\n",
    "            train_x = pd.concat([train_x ,pseudo[features]],axis=0)\n",
    "            train_y = train_y.append(pseudo[target])  \n",
    "            for n, (pseudo_train_idx, pseudo_valid_idx) in enumerate(folds.split(pseudo[features], pseudo['y_bin'])):\n",
    "                print ('PSEUDO FOLD:' + str(n))\n",
    "                if n_fold == n:\n",
    "                    train_x = pd.concat([train_x ,pseudo[features].iloc[pseudo_valid_idx]],axis=0)\n",
    "                    train_y = train_y.append(pseudo[target].iloc[pseudo_valid_idx] )\n",
    "                    break\n",
    "                    \n",
    "        print ('train_x shape:',train_x.shape,train_y.mean())\n",
    "        print ('valid_x shape:',valid_x.shape,valid_y.mean())\n",
    "        \n",
    "        dtrain = lgb.Dataset(train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features) \n",
    "        bst = lgb.train(params, dtrain, num_boost_round=50000,\n",
    "            valid_sets=[dval,dtrain], verbose_eval=500,early_stopping_rounds=500, ) \n",
    "        new_list = sorted(zip(features, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:30]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "         \n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "        oof_cv = rmse(valid_y,  oof_preds[valid_idx])\n",
    "        cv_list.append(oof_cv)\n",
    "        print (cv_list)\n",
    "        sub_preds += bst.predict(test_df[features], num_iteration=bst.best_iteration) / folds.n_splits\n",
    " \n",
    "    cv = rmse(train_df[target],  oof_preds)\n",
    "    print('Full OOF RMSE %.6f' % cv)  \n",
    "\n",
    "    train_df['lgb_y'] = oof_preds\n",
    "    test_df['lgb_y'] = sub_preds\n",
    "    \n",
    "    return train_df,test_df,cv\n",
    "\n",
    "def lgb_kfold2(X_train,X_test,train_df,test_df,features,target,cat_features,folds,params,use_pseudo=False,sampling=False):\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    cv_list = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, train_df['channelTitle'])):\n",
    "        print ('FOLD:' + str(n_fold))\n",
    "        \n",
    "#         train_x, train_y = train_df[features].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "#         valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "\n",
    "        train_x, train_y = X_train[train_idx,:], train_df[target].iloc[train_idx]\n",
    "        valid_x, valid_y = X_train[valid_idx,:], train_df[target].iloc[valid_idx]\n",
    "        \n",
    "        print ('train_x shape:',train_x.shape,train_y.mean())\n",
    "        print ('valid_x shape:',valid_x.shape,valid_y.mean())\n",
    "        \n",
    "        dtrain = lgb.Dataset(train_x, label=train_y,categorical_feature=cat_features)#feature_name=features,\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y,reference=dtrain,categorical_feature=cat_features) # feature_name=features,\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=50000,\n",
    "            valid_sets=[dval,dtrain], verbose_eval=500,early_stopping_rounds=500, ) \n",
    "#         new_list = sorted(zip(features, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:100]\n",
    "#         for item in new_list:\n",
    "#             print (item) \n",
    "         \n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "        oof_cv = rmse(valid_y,  oof_preds[valid_idx])\n",
    "        cv_list.append(oof_cv)\n",
    "        print (cv_list)\n",
    "        sub_preds += bst.predict(X_test, num_iteration=bst.best_iteration) / folds.n_splits\n",
    " \n",
    "    cv = rmse(train_df[target],  oof_preds)\n",
    "    print('Full OOF RMSE %.6f' % cv)  \n",
    "\n",
    "    train_df['prediction'] = oof_preds\n",
    "    test_df['prediction'] = sub_preds\n",
    "    \n",
    "    return train_df,test_df,cv\n",
    "\n",
    "params = {\n",
    "               \"objective\" : \"regression\", #regression\n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": -1,\n",
    "               \"min_data_in_leaf\": 10, #10\n",
    "               \"min_gain_to_split\": 0.01,#0.01\n",
    "                \"min_child_weight\": 0.001,\n",
    "                \"reg_alpha\": 0.1, \n",
    "                \"reg_lambda\": 1, #1\n",
    "               \"num_leaves\" : 31, #50\n",
    "               \"max_bin\" : 300,#300 \n",
    "              \"learning_rate\" :0.1,\n",
    "               \"bagging_fraction\" : 0.8,\n",
    "               \"bagging_freq\" : 1,\n",
    "               \"bagging_seed\" : 4590,\n",
    "               \"feature_fraction\" : 0.7,#0.85\n",
    "               \"verbosity\": -1,\n",
    "               \"boost_from_average\": False,\n",
    "}\n",
    "\n",
    "train_df = df[df['ratings_disabled_encoder']==0]\n",
    "test_df = df[df['ratings_disabled_encoder']==1]\n",
    "train_df['likes'] = np.log1p(train_df['likes'])\n",
    "\n",
    "\n",
    "drop_features=[ 'channelId', 'channelTitle', 'c_date','collection_date','image','comments_ratings_disabled',\n",
    "        'description', 'ratio_title_des','comments_disabled','ratings_disabled',\n",
    "       'id',  'publishedAt',  'tags','new_tags','all_text','new_title', 'new_description',\n",
    "       'thumbnail_link', 'title', 'video_id', 'y','y_bin','lgb_y','comments_ratings',    'dislikes', 'likes',  \n",
    "      'ratings_disabled_encoder', 'comments_ratings_encoder',            \n",
    "              ]\n",
    "\n",
    "\n",
    "features = [f for f in train_df.columns if f not in drop_features]\n",
    "target = 'likes'\n",
    "cat_features = [\n",
    "]\n",
    "\n",
    "\n",
    "seed = 817\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "print ('numerical features:', len(features),features)# \n",
    "\n",
    "train_lgb,test_lgb,cv = lgb_kfold(train_df,test_df,features,target,cat_features,folds,params,use_pseudo=False,sampling=False)\n",
    "train_lgb['likes'] = np.expm1(train_lgb['likes'])\n",
    "train_lgb['likes_pred'] = np.expm1(train_lgb['lgb_y'])\n",
    "test_lgb['likes_pred'] = np.expm1(test_lgb['lgb_y'])\n",
    "\n",
    "train_ = train_lgb[['video_id','likes_pred']]\n",
    "test_ = test_lgb[['video_id','likes_pred']]\n",
    "df_ = pd.concat([train_,test_],axis=0)\n",
    "df_['likes_pred'] = df_['likes_pred'].map(lambda x:0 if x<0 else x)\n",
    "df_[['video_id','likes_pred']].to_csv('../../data/input/probspace/likes_pred_0623.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
