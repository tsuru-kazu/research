{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (19720, 18)\n",
      "test (29582, 16)\n",
      "df (49302, 71)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>channelId</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>description</th>\n",
       "      <th>y</th>\n",
       "      <th>y_bin</th>\n",
       "      <th>comments_ratings</th>\n",
       "      <th>channelId_encoder</th>\n",
       "      <th>channelTitle_encoder</th>\n",
       "      <th>collection_date_encoder</th>\n",
       "      <th>description_encoder</th>\n",
       "      <th>tags_encoder</th>\n",
       "      <th>comments_disabled_encoder</th>\n",
       "      <th>ratings_disabled_encoder</th>\n",
       "      <th>comments_ratings_encoder</th>\n",
       "      <th>dislikes_pred</th>\n",
       "      <th>likes_pred</th>\n",
       "      <th>comment_count_pred</th>\n",
       "      <th>diff_dislikes</th>\n",
       "      <th>diff_likes</th>\n",
       "      <th>diff_comments</th>\n",
       "      <th>original_dislikes</th>\n",
       "      <th>original_likes</th>\n",
       "      <th>original_comment_count</th>\n",
       "      <th>c_date</th>\n",
       "      <th>c_year</th>\n",
       "      <th>c_month</th>\n",
       "      <th>c_day</th>\n",
       "      <th>c_dayofweek</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>seconds_from_publish</th>\n",
       "      <th>days_from_publish</th>\n",
       "      <th>months_from_publish</th>\n",
       "      <th>years_from_publish</th>\n",
       "      <th>days_from_publish_start</th>\n",
       "      <th>days_from_cdate_start</th>\n",
       "      <th>likes_comments</th>\n",
       "      <th>dislikes_comments</th>\n",
       "      <th>comment_count_ratings</th>\n",
       "      <th>comments_ratings_disabled</th>\n",
       "      <th>diff_likes_dislikes</th>\n",
       "      <th>ratio_likes_dislikes</th>\n",
       "      <th>ratio_likes_comment_count</th>\n",
       "      <th>ratio_dislikes_comment_count</th>\n",
       "      <th>likes_by_day</th>\n",
       "      <th>dislikes_by_day</th>\n",
       "      <th>comments_by_day</th>\n",
       "      <th>likes_by_month</th>\n",
       "      <th>dislikes_by_month</th>\n",
       "      <th>comments_by_month</th>\n",
       "      <th>likes_by_year</th>\n",
       "      <th>dislikes_by_year</th>\n",
       "      <th>comments_by_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GDtyztIThRQ</td>\n",
       "      <td>[12] BGM Inazuma Eleven 3 - ~ライオコツト ダンジョン~</td>\n",
       "      <td>2011-01-09 05:50:33+00:00</td>\n",
       "      <td>UCQaNYC3dNvH8FqrEyK7hTJw</td>\n",
       "      <td>DjangoShiny</td>\n",
       "      <td>20</td>\n",
       "      <td>20.01.02</td>\n",
       "      <td>Inazuma|Eleven|Super|Once|bgm|ost|イナズマイレブン|Kyoui|no|Shinryakusha|sekai|he|chosen|challenge|to|the|world|anime|game|ds|music|soundtrack|background|t-pistonz+kmc|berryz|fire|blizzard|spark|bomber|ogre|rip|endou|endo|mark|goenji|kidou|fubuki|aki|kazemaru|someoka|kabeyama|alien|hiroto|midorikawa|song|themes|battle|ffi</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>https://i.ytimg.com/vi/GDtyztIThRQ/default.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>~ライオコツト ダンジョン~Inazuma Eleven 3 BGM Complete (Ripped by Tommy)</td>\n",
       "      <td>29229.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FalseFalse</td>\n",
       "      <td>7498</td>\n",
       "      <td>2223</td>\n",
       "      <td>5</td>\n",
       "      <td>28187</td>\n",
       "      <td>7094</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.37847386</td>\n",
       "      <td>69.14827722</td>\n",
       "      <td>10.95780610</td>\n",
       "      <td>-4.37847386</td>\n",
       "      <td>44.85172278</td>\n",
       "      <td>-3.95780610</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-01 00:00:00+00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>65367</td>\n",
       "      <td>3309</td>\n",
       "      <td>110</td>\n",
       "      <td>9</td>\n",
       "      <td>2086</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.00000000</td>\n",
       "      <td>14.25000000</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.03445150</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.00211544</td>\n",
       "      <td>1.03636364</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.06363636</td>\n",
       "      <td>12.66666667</td>\n",
       "      <td>0.00000000</td>\n",
       "      <td>0.77777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>m4H9s3GtTlQ</td>\n",
       "      <td>ねごと - メルシールー [Official Music Video]</td>\n",
       "      <td>2012-07-23 03:00:09+00:00</td>\n",
       "      <td>UChMWDi-HBm5aS3jyRSaAWUA</td>\n",
       "      <td>ねごと Official Channel</td>\n",
       "      <td>10</td>\n",
       "      <td>20.08.02</td>\n",
       "      <td>ねごと|ネゴト|メルシールー|Re:myend|リマインド|Lightdentity|ライデンティティ|放課後ミッドナイターズ|ナースフル|竹清仁|ループ|シャープ|shrap ♯|蒼山幸子|澤村小夜子|藤咲佑|沙田瑞紀|Hello! Z|ex Negoto</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>https://i.ytimg.com/vi/m4H9s3GtTlQ/default.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.negoto.com/全員平成生まれ、蒼山幸子（Vo＆Key)、沙田瑞紀（G)、藤咲佑（Ba）、澤村小夜子（Dr）からなるオルタナティブでファンタジックなロックを鳴らすガールズ4ピースバンド＜ねごと＞。2011年7月13日リリースの1st Full Album「ex Negoto」より「メルシールー」。孤独になってしまったとき、胸の奥にしまっていた大切なことを思い出せたら、すべてがまぶしく見える、そんな瞬間を描いた歌詞と相成る楽曲はシンセ音がエッセンスとなったライブでも盛り上がり必至のナンバー。</td>\n",
       "      <td>730280.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FalseFalse</td>\n",
       "      <td>12289</td>\n",
       "      <td>15874</td>\n",
       "      <td>12</td>\n",
       "      <td>26010</td>\n",
       "      <td>29252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.04819316</td>\n",
       "      <td>3159.78843121</td>\n",
       "      <td>102.95138114</td>\n",
       "      <td>-14.04819316</td>\n",
       "      <td>-274.78843121</td>\n",
       "      <td>8.04861886</td>\n",
       "      <td>50</td>\n",
       "      <td>2885</td>\n",
       "      <td>111</td>\n",
       "      <td>2020-02-08 00:00:00+00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>75591</td>\n",
       "      <td>2755</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>2646</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2835.0</td>\n",
       "      <td>56.56862745</td>\n",
       "      <td>25.75892857</td>\n",
       "      <td>0.44642857</td>\n",
       "      <td>1.04718693</td>\n",
       "      <td>0.01814882</td>\n",
       "      <td>0.04029038</td>\n",
       "      <td>31.70329670</td>\n",
       "      <td>0.54945055</td>\n",
       "      <td>1.21978022</td>\n",
       "      <td>412.14285714</td>\n",
       "      <td>7.14285714</td>\n",
       "      <td>15.85714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>z19zYZuLuEU</td>\n",
       "      <td>VF3tb 闇よだれvsちび太 (SEGA)</td>\n",
       "      <td>2007-07-26 13:54:09+00:00</td>\n",
       "      <td>UCBdcyoZSt5HBLd_n6we-xIg</td>\n",
       "      <td>siropai</td>\n",
       "      <td>24</td>\n",
       "      <td>20.14.01</td>\n",
       "      <td>VF3|VF4|VF5|ちび太|闇よだれ|chibita|virtuafighter|sega|バーチャ</td>\n",
       "      <td>133.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>https://i.ytimg.com/vi/z19zYZuLuEU/default.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Beat-tribe cup finalhttp://ameblo.jp/siropai/</td>\n",
       "      <td>80667.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FalseFalse</td>\n",
       "      <td>3392</td>\n",
       "      <td>14185</td>\n",
       "      <td>16</td>\n",
       "      <td>5711</td>\n",
       "      <td>16068</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.45271986</td>\n",
       "      <td>85.10976147</td>\n",
       "      <td>21.57864536</td>\n",
       "      <td>10.54728014</td>\n",
       "      <td>47.89023853</td>\n",
       "      <td>-7.57864536</td>\n",
       "      <td>17</td>\n",
       "      <td>133</td>\n",
       "      <td>14</td>\n",
       "      <td>2020-01-14 00:00:00+00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>54</td>\n",
       "      <td>36351</td>\n",
       "      <td>4554</td>\n",
       "      <td>151</td>\n",
       "      <td>12</td>\n",
       "      <td>823</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>7.38888889</td>\n",
       "      <td>8.86666667</td>\n",
       "      <td>1.13333333</td>\n",
       "      <td>0.02920509</td>\n",
       "      <td>0.00373298</td>\n",
       "      <td>0.00307422</td>\n",
       "      <td>0.88079470</td>\n",
       "      <td>0.11258278</td>\n",
       "      <td>0.09271523</td>\n",
       "      <td>11.08333333</td>\n",
       "      <td>1.41666667</td>\n",
       "      <td>1.16666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>pmcIOsL7s98</td>\n",
       "      <td>free frosty weekend!</td>\n",
       "      <td>2005-05-15 02:38:43+00:00</td>\n",
       "      <td>UC7K5am1UAQEsCRhzXpi9i1g</td>\n",
       "      <td>Jones4Carrie</td>\n",
       "      <td>22</td>\n",
       "      <td>19.22.12</td>\n",
       "      <td>frosty</td>\n",
       "      <td>287.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>https://i.ytimg.com/vi/pmcIOsL7s98/default.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I look so bad  but look at me!</td>\n",
       "      <td>34826.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FalseFalse</td>\n",
       "      <td>2243</td>\n",
       "      <td>3922</td>\n",
       "      <td>0</td>\n",
       "      <td>11020</td>\n",
       "      <td>20320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.44158934</td>\n",
       "      <td>427.02746361</td>\n",
       "      <td>125.20247855</td>\n",
       "      <td>24.55841066</td>\n",
       "      <td>-140.02746361</td>\n",
       "      <td>47.79752145</td>\n",
       "      <td>51</td>\n",
       "      <td>287</td>\n",
       "      <td>173</td>\n",
       "      <td>2019-12-22 00:00:00+00:00</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2005</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>76877</td>\n",
       "      <td>5333</td>\n",
       "      <td>177</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>5.51923077</td>\n",
       "      <td>1.64942529</td>\n",
       "      <td>0.29310345</td>\n",
       "      <td>0.05381586</td>\n",
       "      <td>0.00956310</td>\n",
       "      <td>0.03243953</td>\n",
       "      <td>1.62146893</td>\n",
       "      <td>0.28813559</td>\n",
       "      <td>0.97740113</td>\n",
       "      <td>20.50000000</td>\n",
       "      <td>3.64285714</td>\n",
       "      <td>12.35714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ZuQgsTcuM-4</td>\n",
       "      <td>トップ・オブ・ザ・ワールド</td>\n",
       "      <td>2007-09-09 09:52:47+00:00</td>\n",
       "      <td>UCTW1um4R-QWa8iIfITGvlZQ</td>\n",
       "      <td>Tatsuya Maruyama</td>\n",
       "      <td>10</td>\n",
       "      <td>20.08.01</td>\n",
       "      <td>ギター|guitar|南澤大介|トップオブザワールド|トップ|オブ|ワールド|カーペンターズ|クラシックギター|ソロギターのしらべ|まるちゃん0208|まるやまたつや</td>\n",
       "      <td>178.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>https://i.ytimg.com/vi/ZuQgsTcuM-4/default.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ソロギターのしらべより「トップオブザワールド」です。クラシックギターで弾いてます。Official Website 【http://maruyama-tatsuya.jimdo.com/】Twitter【https://twitter.com/TatsuyaMaruyama】</td>\n",
       "      <td>172727.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FalseFalse</td>\n",
       "      <td>8306</td>\n",
       "      <td>7711</td>\n",
       "      <td>11</td>\n",
       "      <td>34917</td>\n",
       "      <td>30615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.01195976</td>\n",
       "      <td>248.24972466</td>\n",
       "      <td>23.09445616</td>\n",
       "      <td>-5.01195976</td>\n",
       "      <td>-70.24972466</td>\n",
       "      <td>-6.09445616</td>\n",
       "      <td>6</td>\n",
       "      <td>178</td>\n",
       "      <td>17</td>\n",
       "      <td>2020-01-08 00:00:00+00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>50833</td>\n",
       "      <td>4503</td>\n",
       "      <td>150</td>\n",
       "      <td>12</td>\n",
       "      <td>868</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>25.42857143</td>\n",
       "      <td>9.88888889</td>\n",
       "      <td>0.33333333</td>\n",
       "      <td>0.03952920</td>\n",
       "      <td>0.00133245</td>\n",
       "      <td>0.00377526</td>\n",
       "      <td>1.18666667</td>\n",
       "      <td>0.04000000</td>\n",
       "      <td>0.11333333</td>\n",
       "      <td>14.83333333</td>\n",
       "      <td>0.50000000</td>\n",
       "      <td>1.41666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     video_id                                       title               publishedAt                 channelId          channelTitle  categoryId collection_date                                                                                                                                                                                                                                                                                                                         tags   likes  dislikes  comment_count                                  thumbnail_link  comments_disabled  ratings_disabled                                                                                                                                                                                                                                                            description         y  y_bin comments_ratings  channelId_encoder  channelTitle_encoder  collection_date_encoder  description_encoder  tags_encoder  \\\n",
       "0   1  GDtyztIThRQ  [12] BGM Inazuma Eleven 3 - ~ライオコツト ダンジョン~ 2011-01-09 05:50:33+00:00  UCQaNYC3dNvH8FqrEyK7hTJw           DjangoShiny          20        20.01.02  Inazuma|Eleven|Super|Once|bgm|ost|イナズマイレブン|Kyoui|no|Shinryakusha|sekai|he|chosen|challenge|to|the|world|anime|game|ds|music|soundtrack|background|t-pistonz+kmc|berryz|fire|blizzard|spark|bomber|ogre|rip|endou|endo|mark|goenji|kidou|fubuki|aki|kazemaru|someoka|kabeyama|alien|hiroto|midorikawa|song|themes|battle|ffi   114.0       0.0            7.0  https://i.ytimg.com/vi/GDtyztIThRQ/default.jpg                  0                 0                                                                                                                                                                                                          ~ライオコツト ダンジョン~Inazuma Eleven 3 BGM Complete (Ripped by Tommy)   29229.0    5.0       FalseFalse               7498                  2223                        5                28187          7094   \n",
       "1   2  m4H9s3GtTlQ         ねごと - メルシールー [Official Music Video] 2012-07-23 03:00:09+00:00  UChMWDi-HBm5aS3jyRSaAWUA  ねごと Official Channel          10        20.08.02                                                                                                                                                                                            ねごと|ネゴト|メルシールー|Re:myend|リマインド|Lightdentity|ライデンティティ|放課後ミッドナイターズ|ナースフル|竹清仁|ループ|シャープ|shrap ♯|蒼山幸子|澤村小夜子|藤咲佑|沙田瑞紀|Hello! Z|ex Negoto  2885.0      50.0          111.0  https://i.ytimg.com/vi/m4H9s3GtTlQ/default.jpg                  0                 0  http://www.negoto.com/全員平成生まれ、蒼山幸子（Vo＆Key)、沙田瑞紀（G)、藤咲佑（Ba）、澤村小夜子（Dr）からなるオルタナティブでファンタジックなロックを鳴らすガールズ4ピースバンド＜ねごと＞。2011年7月13日リリースの1st Full Album「ex Negoto」より「メルシールー」。孤独になってしまったとき、胸の奥にしまっていた大切なことを思い出せたら、すべてがまぶしく見える、そんな瞬間を描いた歌詞と相成る楽曲はシンセ音がエッセンスとなったライブでも盛り上がり必至のナンバー。  730280.0    6.0       FalseFalse              12289                 15874                       12                26010         29252   \n",
       "2   3  z19zYZuLuEU                      VF3tb 闇よだれvsちび太 (SEGA) 2007-07-26 13:54:09+00:00  UCBdcyoZSt5HBLd_n6we-xIg               siropai          24        20.14.01                                                                                                                                                                                                                                                                         VF3|VF4|VF5|ちび太|闇よだれ|chibita|virtuafighter|sega|バーチャ   133.0      17.0           14.0  https://i.ytimg.com/vi/z19zYZuLuEU/default.jpg                  0                 0                                                                                                                                                                                                                          Beat-tribe cup finalhttp://ameblo.jp/siropai/   80667.0    5.0       FalseFalse               3392                 14185                       16                 5711         16068   \n",
       "3   4  pmcIOsL7s98                        free frosty weekend! 2005-05-15 02:38:43+00:00  UC7K5am1UAQEsCRhzXpi9i1g          Jones4Carrie          22        19.22.12                                                                                                                                                                                                                                                                                                                       frosty   287.0      51.0          173.0  https://i.ytimg.com/vi/pmcIOsL7s98/default.jpg                  0                 0                                                                                                                                                                                                                                         I look so bad  but look at me!   34826.0    5.0       FalseFalse               2243                  3922                        0                11020         20320   \n",
       "4   5  ZuQgsTcuM-4                               トップ・オブ・ザ・ワールド 2007-09-09 09:52:47+00:00  UCTW1um4R-QWa8iIfITGvlZQ      Tatsuya Maruyama          10        20.08.01                                                                                                                                                                                                                                          ギター|guitar|南澤大介|トップオブザワールド|トップ|オブ|ワールド|カーペンターズ|クラシックギター|ソロギターのしらべ|まるちゃん0208|まるやまたつや   178.0       6.0           17.0  https://i.ytimg.com/vi/ZuQgsTcuM-4/default.jpg                  0                 0                                                                                                                             ソロギターのしらべより「トップオブザワールド」です。クラシックギターで弾いてます。Official Website 【http://maruyama-tatsuya.jimdo.com/】Twitter【https://twitter.com/TatsuyaMaruyama】  172727.0    6.0       FalseFalse               8306                  7711                       11                34917         30615   \n",
       "\n",
       "   comments_disabled_encoder  ratings_disabled_encoder  comments_ratings_encoder  dislikes_pred     likes_pred  comment_count_pred  diff_dislikes    diff_likes  diff_comments  original_dislikes  original_likes  original_comment_count                    c_date  c_year  c_month  c_day  c_dayofweek  year  month  weekofyear  day  dayofweek  hour  minute  seconds_from_publish  days_from_publish  months_from_publish  years_from_publish  days_from_publish_start  days_from_cdate_start  likes_comments  dislikes_comments  comment_count_ratings  comments_ratings_disabled  diff_likes_dislikes  ratio_likes_dislikes  ratio_likes_comment_count  ratio_dislikes_comment_count  likes_by_day  dislikes_by_day  comments_by_day  likes_by_month  dislikes_by_month  comments_by_month  likes_by_year  dislikes_by_year  comments_by_year  \n",
       "0                          0                         0                         0     4.37847386    69.14827722         10.95780610    -4.37847386   44.85172278    -3.95780610                  0             114                       7 2020-02-01 00:00:00+00:00    2020        2      1            5  2011      1           1    9          6     5      50                 65367               3309                  110                   9                     2086                     41             0.0                0.0                    0.0                          0                114.0          114.00000000                14.25000000                    0.00000000    0.03445150       0.00000000       0.00211544      1.03636364         0.00000000         0.06363636    12.66666667        0.00000000        0.77777778  \n",
       "1                          0                         0                         0    64.04819316  3159.78843121        102.95138114   -14.04819316 -274.78843121     8.04861886                 50            2885                     111 2020-02-08 00:00:00+00:00    2020        2      8            5  2012      7          30   23          0     3       0                 75591               2755                   91                   7                     2646                     48             0.0                0.0                    0.0                          0               2835.0           56.56862745                25.75892857                    0.44642857    1.04718693       0.01814882       0.04029038     31.70329670         0.54945055         1.21978022   412.14285714        7.14285714       15.85714286  \n",
       "2                          0                         0                         0     6.45271986    85.10976147         21.57864536    10.54728014   47.89023853    -7.57864536                 17             133                      14 2020-01-14 00:00:00+00:00    2020        1     14            1  2007      7          30   26          3    13      54                 36351               4554                  151                  12                      823                     23             0.0                0.0                    0.0                          0                116.0            7.38888889                 8.86666667                    1.13333333    0.02920509       0.00373298       0.00307422      0.88079470         0.11258278         0.09271523    11.08333333        1.41666667        1.16666667  \n",
       "3                          0                         0                         0    26.44158934   427.02746361        125.20247855    24.55841066 -140.02746361    47.79752145                 51             287                     173 2019-12-22 00:00:00+00:00    2019       12     22            6  2005      5          19   15          6     2      38                 76877               5333                  177                  14                       20                      0             0.0                0.0                    0.0                          0                236.0            5.51923077                 1.64942529                    0.29310345    0.05381586       0.00956310       0.03243953      1.62146893         0.28813559         0.97740113    20.50000000        3.64285714       12.35714286  \n",
       "4                          0                         0                         0    11.01195976   248.24972466         23.09445616    -5.01195976  -70.24972466    -6.09445616                  6             178                      17 2020-01-08 00:00:00+00:00    2020        1      8            2  2007      9          36    9          6     9      52                 50833               4503                  150                  12                      868                     17             0.0                0.0                    0.0                          0                172.0           25.42857143                 9.88888889                    0.33333333    0.03952920       0.00133245       0.00377526      1.18666667         0.04000000         0.11333333    14.83333333        0.50000000        1.41666667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['id', 'video_id', 'title', 'publishedAt', 'channelId',\n",
       "       'channelTitle', 'categoryId', 'collection_date', 'tags', 'likes',\n",
       "       'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled',\n",
       "       'ratings_disabled', 'description', 'y', 'y_bin',\n",
       "       'comments_ratings', 'channelId_encoder', 'channelTitle_encoder',\n",
       "       'collection_date_encoder', 'description_encoder', 'tags_encoder',\n",
       "       'comments_disabled_encoder', 'ratings_disabled_encoder',\n",
       "       'comments_ratings_encoder', 'dislikes_pred', 'likes_pred',\n",
       "       'comment_count_pred', 'diff_dislikes', 'diff_likes',\n",
       "       'diff_comments', 'original_dislikes', 'original_likes',\n",
       "       'original_comment_count', 'c_date', 'c_year', 'c_month', 'c_day',\n",
       "       'c_dayofweek', 'year', 'month', 'weekofyear', 'day', 'dayofweek',\n",
       "       'hour', 'minute', 'seconds_from_publish', 'days_from_publish',\n",
       "       'months_from_publish', 'years_from_publish',\n",
       "       'days_from_publish_start', 'days_from_cdate_start',\n",
       "       'likes_comments', 'dislikes_comments', 'comment_count_ratings',\n",
       "       'comments_ratings_disabled', 'diff_likes_dislikes',\n",
       "       'ratio_likes_dislikes', 'ratio_likes_comment_count',\n",
       "       'ratio_dislikes_comment_count', 'likes_by_day', 'dislikes_by_day',\n",
       "       'comments_by_day', 'likes_by_month', 'dislikes_by_month',\n",
       "       'comments_by_month', 'likes_by_year', 'dislikes_by_year',\n",
       "       'comments_by_year'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 46.9 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "# import tensorflow as tf\n",
    "import MeCab \n",
    "import re\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "# from matplotlib_venn import venn2\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option(\"display.precision\", 8)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "    \n",
    "# seed\n",
    "seed = 817\n",
    "seed_everything(seed)\n",
    "\n",
    "# load train test\n",
    "train = pd.read_csv('../../data/input/probspace/train_data.csv')\n",
    "train['y_bin'] = pd.cut(train['y'], [0, 10, 100,1000,10000,100000,1000000,10000000000], labels=[1,2,3,4,5,6,7])\n",
    "train['y_bin'] = train['y_bin'].astype(int)\n",
    "test = pd.read_csv('../../data/input/probspace/test_data.csv')\n",
    "df = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "print ('train',train.shape)\n",
    "print ('test',test.shape)\n",
    "df['comments_ratings'] = df['comments_disabled'].astype(str)+df['ratings_disabled'].astype(str)\n",
    "    \n",
    "for c in ['channelId','channelTitle','collection_date','description','tags','comments_disabled','ratings_disabled','comments_ratings']:\n",
    "    lbl = LabelEncoder()\n",
    "    df[c+'_encoder'] = lbl.fit_transform(df[c].astype(str))    \n",
    "    \n",
    "# use predicted dislikes,likes,comment_out\n",
    "dislikes_pred = pd.read_csv('../../data/input/probspace/dislikes_pred_0623.csv')\n",
    "likes_pred = pd.read_csv('../../data/input/probspace/likes_pred_0623.csv')\n",
    "comments_pred = pd.read_csv('../../data/input/probspace/comment_count_pred_0623.csv')\n",
    "df = df.merge(dislikes_pred,on=['video_id'],how='left')\n",
    "df = df.merge(likes_pred,on=['video_id'],how='left')\n",
    "df = df.merge(comments_pred,on=['video_id'],how='left')\n",
    "df['diff_dislikes'] = df['dislikes'] - df['dislikes_pred']\n",
    "df['diff_likes'] = df['likes'] - df['likes_pred']\n",
    "df['diff_comments'] = df['comment_count'] - df['comment_count_pred']\n",
    "df['original_dislikes'] = df['dislikes']\n",
    "df['original_likes'] = df['likes']\n",
    "df['original_comment_count'] = df['comment_count']\n",
    "df.loc[df['ratings_disabled']==True,'dislikes'] = df.loc[df['ratings_disabled']==True,'dislikes_pred']\n",
    "df.loc[df['ratings_disabled']==True,'likes'] = df.loc[df['ratings_disabled']==True,'likes_pred']\n",
    "df.loc[df['comments_disabled']==True,'comment_count'] = df.loc[df['comments_disabled']==True,'comment_count_pred']\n",
    "\n",
    "# timestamp transformation\n",
    "df[\"c_date\"] = \"20\" + df[\"collection_date\"]\n",
    "df[\"c_date\"] = pd.to_datetime(df[\"c_date\"], utc=True, format=\"%Y.%d.%m\")\n",
    "df[\"c_year\"] = df[\"c_date\"].dt.year\n",
    "df[\"c_month\"] = df[\"c_date\"].dt.month\n",
    "df[\"c_day\"] = df[\"c_date\"].dt.day\n",
    "df[\"c_dayofweek\"] = df[\"c_date\"].dt.dayofweek\n",
    "\n",
    "df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"],utc=True, format=\"%Y-%m-%d\")\n",
    "df[\"year\"] = df[\"publishedAt\"].dt.year\n",
    "df[\"month\"] = df[\"publishedAt\"].dt.month\n",
    "df[\"weekofyear\"] = df[\"publishedAt\"].dt.weekofyear\n",
    "df[\"day\"] = df[\"publishedAt\"].dt.day\n",
    "df[\"dayofweek\"] = df[\"publishedAt\"].dt.dayofweek\n",
    "df[\"hour\"] = df[\"publishedAt\"].dt.hour\n",
    "df[\"minute\"] = df[\"publishedAt\"].dt.minute    \n",
    "\n",
    "df['seconds_from_publish'] = (df['c_date'] - df['publishedAt']).dt.seconds\n",
    "df['days_from_publish'] = (df['c_date'] - df['publishedAt']).dt.days\n",
    "df['months_from_publish'] = (df['c_date'] - df['publishedAt']).dt.days // 30\n",
    "df['years_from_publish'] = (df['c_date'] - df['publishedAt']).dt.days // 365\n",
    "\n",
    "df['days_from_publish_start'] = (df['publishedAt'] - df['publishedAt'].min()).dt.days\n",
    "df['days_from_cdate_start'] = (df['c_date'] - df['c_date'].min()).dt.days\n",
    "\n",
    "# interaction \n",
    "df['comments_disabled'] = df['comments_disabled'].map(lambda x:1 if x==True else 0)\n",
    "df['ratings_disabled'] = df['ratings_disabled'].map(lambda x:1 if x==True else 0)\n",
    "\n",
    "df['likes_comments'] = df['likes'] * df['comments_disabled']\n",
    "df['dislikes_comments'] = df['dislikes'] * df['comments_disabled']\n",
    "df['comment_count_ratings'] = df['comment_count'] * df['ratings_disabled']\n",
    "\n",
    "df['comments_ratings_disabled'] = df['comments_disabled'] + df['ratings_disabled']\n",
    "df['diff_likes_dislikes'] = df['likes'] - df['dislikes'] \n",
    "df['ratio_likes_dislikes'] = df['likes'] / (df['dislikes'] + 1)\n",
    "df['ratio_likes_comment_count'] = df['likes'] / (df['comment_count'] + 1)\n",
    "df['ratio_dislikes_comment_count'] = df['dislikes'] / (df['comment_count'] + 1)\n",
    "\n",
    "df['likes_by_day'] = df['likes'] / df['days_from_publish']\n",
    "df['dislikes_by_day'] = df['dislikes'] / df['days_from_publish']\n",
    "df['comments_by_day'] = df['comment_count'] / df['days_from_publish']\n",
    "\n",
    "df['likes_by_month'] = df['likes'] / df['months_from_publish']\n",
    "df['dislikes_by_month'] = df['dislikes'] / df['months_from_publish']\n",
    "df['comments_by_month'] = df['comment_count'] / df['months_from_publish']\n",
    "\n",
    "df['likes_by_year'] = df['likes'] / df['years_from_publish']\n",
    "df['dislikes_by_year'] = df['dislikes'] / df['years_from_publish']\n",
    "df['comments_by_year'] = df['comment_count'] / df['years_from_publish']\n",
    "\n",
    "print ('df',df.shape)\n",
    "display(df.head())\n",
    "display(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.9 s, sys: 234 ms, total: 37.2 s\n",
      "Wall time: 38.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.decomposition import NMF,LatentDirichletAllocation,TruncatedSVD\n",
    "from gensim.sklearn_api.ldamodel import LdaTransformer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "import unicodedata\n",
    "\n",
    "class MecabTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wakati = MeCab.Tagger('-Owakati')\n",
    "        self.wakati.parse('')\n",
    "\n",
    "    def tokenize(self, line):\n",
    "        txt = self.wakati.parse(line)\n",
    "        txt = txt.split()\n",
    "        return txt\n",
    "    \n",
    "    def mecab_tokenizer(self, line):\n",
    "        node = self.wakati.parseToNode(line)\n",
    "        keywords = []\n",
    "        while node:\n",
    "            if node.feature.split(\",\")[0] == \"名詞\" or node.feature.split(\",\")[0] == \"形容詞\":\n",
    "                keywords.append(node.surface)\n",
    "            node = node.next\n",
    "        return keywords    \n",
    "    \n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "html_tags = ['<p>', '</p>', '<table>', '</table>', '<tr>', '</tr>', '<ul>', '<ol>', '<dl>', '</ul>', '</ol>',\n",
    "             '</dl>', '<li>', '<dd>', '<dt>', '</li>', '</dd>', '</dt>', '<h1>', '</h1>',\n",
    "             '<br>', '<br/>', '<strong>', '</strong>', '<span>', '</span>', '<blockquote>', '</blockquote>',\n",
    "             '<pre>', '</pre>', '<div>', '</div>', '<h2>', '</h2>', '<h3>', '</h3>', '<h4>', '</h4>', '<h5>', '</h5>',\n",
    "             '<h6>', '</h6>', '<blck>', '<pr>', '<code>', '<th>', '</th>', '<td>', '</td>', '<em>', '</em>']\n",
    "\n",
    "empty_expressions = ['&lt;', '&gt;', '&amp;', '&nbsp;', \n",
    "                     '&emsp;', '&ndash;', '&mdash;', '&ensp;'\n",
    "                     '&quot;', '&#39;']\n",
    "\n",
    "other = ['span', 'style', 'href', 'input']\n",
    "\n",
    "\n",
    "def pre_preprocess(x):\n",
    "    return str(x).lower()\n",
    "\n",
    "def rm_spaces(text):\n",
    "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u2009', '\\u2028', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\u3000', '\\x10', '\\x7f', '\\x9d', '\\xad',\n",
    "              '\\x97', '\\x9c', '\\x8b', '\\x81', '\\x80', '\\x8c', '\\x85', '\\x92', '\\x88', '\\x8d', '\\x80', '\\x8e', '\\x9a', '\\x94', '\\xa0', \n",
    "              '\\x8f', '\\x82', '\\x8a', '\\x93', '\\x90', '\\x83', '\\x96', '\\x9b', '\\x9e', '\\x99', '\\x87', '\\x84', '\\x9f',\n",
    "             ]\n",
    "    for space in spaces:\n",
    "            text = text.replace(space, ' ')\n",
    "    return text\n",
    "\n",
    "def remove_urls(x):\n",
    "    x = re.sub(r'(https?://[a-zA-Z0-9.-]*)', r'', x)\n",
    "\n",
    "    # original\n",
    "    x = re.sub(r'(quote=\\w+\\s?\\w+;?\\w+)', r'', x)\n",
    "    return x\n",
    "\n",
    "def clean_html_tags(x, stop_words=[]):      \n",
    "    for r in html_tags:\n",
    "        x = x.replace(r, '')\n",
    "    for r in empty_expressions:\n",
    "        x = x.replace(r, ' ')\n",
    "    for r in stop_words:\n",
    "        x = x.replace(r, '')\n",
    "    return x\n",
    "\n",
    "def replace_num(text):\n",
    "    text = re.sub('[0-9]{5,}', '', text)\n",
    "    text = re.sub('[0-9]{4}', '', text)\n",
    "    text = re.sub('[0-9]{3}', '', text)\n",
    "    text = re.sub('[0-9]{2}', '', text)\n",
    "    return text\n",
    "\n",
    "def get_url_num(x):\n",
    "    pattern = \"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\"\n",
    "    urls = re.findall(pattern, x)\n",
    "    return len(urls)\n",
    "\n",
    "\n",
    "def clean_puncts(x):\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "#zenkaku = '０,１,２,３,４,５,６,７,８,９,（,）,＊,「,」,［,］,【,】,＜,＞,？,・,＃,＠,＄,％,＝'.split(',')\n",
    "#hankaku = '0,1,2,3,4,5,6,7,8,9,q,a,z,w,s,x,c,d,e,r,f,v,b,g,t,y,h,n,m,j,u,i,k,l,o,p'.split(',')\n",
    "\n",
    "def clean_text_jp(x):\n",
    "    x = x.replace('。', '')\n",
    "    x = x.replace('、', '')\n",
    "    x = x.replace('\\n', '') # 改行削除\n",
    "    x = x.replace('\\t', '') # タブ削除\n",
    "    x = x.replace('\\r', '')\n",
    "    x = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', x) \n",
    "    x = re.sub(r'\\[math\\]', ' LaTex math ', x) # LaTex削除\n",
    "    x = re.sub(r'\\[\\/math\\]', ' LaTex math ', x) # LaTex削除\n",
    "    x = re.sub(r'\\\\', ' LaTex ', x) # LaTex削除   \n",
    "    #for r in zenkaku+hankaku:\n",
    "    #    x = x.replace(str(r), '')\n",
    "    x = re.sub(' +', ' ', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = data.apply(lambda x: pre_preprocess(x))\n",
    "    data = data.apply(lambda x: rm_spaces(x))\n",
    "    data = data.apply(lambda x: remove_urls(x))\n",
    "    data = data.apply(lambda x: clean_puncts(x))\n",
    "   # data = data.apply(lambda x: replace_num(x))\n",
    "    data = data.apply(lambda x: clean_html_tags(x, stop_words=other))\n",
    "    data = data.apply(lambda x: clean_text_jp(x))\n",
    "    return data    \n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "def is_japanese(string):\n",
    "    for ch in string:\n",
    "        try:\n",
    "            name = unicodedata.name(ch) \n",
    "            if \"CJK UNIFIED\" in name \\\n",
    "            or \"HIRAGANA\" in name \\\n",
    "            or \"KATAKANA\" in name:\n",
    "                return True\n",
    "        except:\n",
    "          continue\n",
    "    return False\n",
    "\n",
    "stopwords = {x: 1 for x in stopwords.words('english')}\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "df['new_tags'] = df['tags'].astype(str).apply(lambda x: x.replace('|',' '))\n",
    "df['all_text'] =  (df['channelTitle'].fillna('') + ' ' + df['description'].fillna('') + ' ' + df['title'].fillna('')+ ' ' + df['new_tags'].fillna('')).astype(str)\n",
    "df['all_text'] = preprocess(df['all_text'])\n",
    "text_cols = ['channelTitle','description','title','new_tags','all_text']\n",
    "for cols in text_cols:   \n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols + '_num_cap'] = df[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))\n",
    "    df[cols + '_num_low'] = df[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    df[cols + '_num_dig'] = df[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    df[cols + '_num_engdig'] = df[cols].apply(lambda x: count_regexp_occ('[A-Za-z0-9]', x))    \n",
    "    df[cols + '_isja'] = df[cols].apply(lambda x: 1 if is_japanese(x) else 0)\n",
    "    df[cols + '_isalpha'] = df[cols].apply(lambda x: 1 if x.encode('utf-8').isalnum() else 0)\n",
    "    \n",
    "    df[cols + '_num_pun'] = df[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    df[cols + '_num_space'] = df[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    \n",
    "    df[cols + '_ratio_unique_words'] = df[cols+'_num_unique_words'] / (df[cols+'_num_words']+1) # Count Unique Words    \n",
    "\n",
    "    df[cols +'_num_stopwords'] = df[cols].apply(lambda x: len([w for w in x.split() if w in stopwords]))\n",
    "    df[cols +'_num_words_upper'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    df[cols +'_num_words_lower'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    df[cols +'_num_words_title'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "       \n",
    "for cols in ['description']:  \n",
    "    df[cols] = df[cols].astype(str)     \n",
    "    df[cols + '_url_num'] = df[cols].apply(lambda x: get_url_num(x))      \n",
    "\n",
    "# for cols in ['channelTitle','title','all_text']:   \n",
    "#     df[cols] = df[cols].astype(str) \n",
    "#     df[cols + '_num_cap'] = df[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))\n",
    "#     df[cols + '_num_low'] = df[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "#     df[cols + '_num_dig'] = df[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "#     df[cols + '_num_engdig'] = df[cols].apply(lambda x: count_regexp_occ('[A-Za-z0-9]', x))    \n",
    "#     df[cols + '_isja'] = df[cols].apply(lambda x: 1 if is_japanese(x) else 0)\n",
    "#     df[cols + '_isalpha'] = df[cols].apply(lambda x: 1 if x.encode('utf-8').isalnum() else 0)\n",
    "#     df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "#     df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "#     df[cols +'_num_stopwords'] = df[cols].apply(lambda x: len([w for w in x.split() if w in stopwords]))\n",
    "#     df[cols +'_num_words_title'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "# for cols in ['new_tags']:       \n",
    "#     df[cols] = df[cols].astype(str) \n",
    "#     df[cols + '_num_engdig'] = df[cols].apply(lambda x: count_regexp_occ('[A-Za-z0-9]', x))    \n",
    "#     df[cols + '_isja'] = df[cols].apply(lambda x: 1 if is_japanese(x) else 0)\n",
    "#     df[cols + '_isalpha'] = df[cols].apply(lambda x: 1 if x.encode('utf-8').isalnum() else 0)\n",
    "#     df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "#     df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "#     df[cols +'_num_words_title'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#     df[cols + '_num_cap'] = df[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))\n",
    "#     df[cols + '_num_low'] = df[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "#     df[cols + '_num_dig'] = df[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "#     df[cols + '_num_engdig'] = df[cols].apply(lambda x: count_regexp_occ('[A-Za-z0-9]', x))    \n",
    "#     df[cols + '_isja'] = df[cols].apply(lambda x: 1 if is_japanese(x) else 0)\n",
    "#     df[cols + '_isalpha'] = df[cols].apply(lambda x: 1 if x.encode('utf-8').isalnum() else 0)\n",
    "    \n",
    "#     df[cols + '_num_pun'] = df[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "#     df[cols + '_num_space'] = df[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "\n",
    "#     df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "#     df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "#     df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    \n",
    "#     df[cols + '_ratio_unique_words'] = df[cols+'_num_unique_words'] / (df[cols+'_num_words']+1) # Count Unique Words    \n",
    "\n",
    "#     df[cols +'_num_stopwords'] = df[cols].apply(lambda x: len([w for w in x.split() if w in stopwords]))\n",
    "#     df[cols +'_num_words_upper'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#     df[cols +'_num_words_lower'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "#     df[cols +'_num_words_title'] = df[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channelTitle\n",
      "description\n",
      "title\n",
      "all_text\n",
      "new_tags\n",
      "CPU times: user 1min 5s, sys: 20.9 s, total: 1min 26s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### TFIDF Vectorizer ###\n",
    "### SVD Components ###\n",
    "n_comp = 20\n",
    "\n",
    "for i in ['channelTitle','description','title','all_text']:#,'new_title','new_description',\n",
    "    print (i)\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,2))\n",
    "    text_tfidf = tfidf_vec.fit_transform(df[i].values.tolist() )\n",
    "    text_svd = TruncatedSVD(n_components=n_comp, algorithm='arpack',random_state=9999)\n",
    "    df_svd = pd.DataFrame(text_svd.fit_transform(text_tfidf))\n",
    "    df_svd.columns = ['svd_'+str(i)+str(j+1) for j in range(n_comp)]\n",
    "    df = pd.concat([df,df_svd],axis=1)\n",
    "    \n",
    "for i in ['new_tags',]:\n",
    "    print (i)\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    text_tfidf = tfidf_vec.fit_transform(df[i].values.tolist() )\n",
    "    text_svd = TruncatedSVD(n_components=n_comp, algorithm='arpack',random_state=9999)\n",
    "    df_svd = pd.DataFrame(text_svd.fit_transform(text_tfidf))\n",
    "    df_svd.columns = ['svd_char_'+str(i)+str(j+1) for j in range(n_comp)]\n",
    "    df = pd.concat([df,df_svd],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 79/161 [00:00<00:00, 161.04it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: diff_likes_dislikes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36magg\u001b[0;34m(df, agg_cols)\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/research-chJ7-kg6/lib/python3.8/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             )\n\u001b[0;32m-> 1650\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/research-chJ7-kg6/lib/python3.8/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column not found: {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: diff_likes_dislikes'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "def agg(df,agg_cols):\n",
    "    for c in tqdm(agg_cols):\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(c['groupby']), c['agg'], c['target'])\n",
    "        if c['agg'] == 'diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.diff(c['para1']).shift(c['para2']))\n",
    "        elif c['agg'] == 'lag':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].shift(c['para1'])\n",
    "        elif c['agg'] == 'rolling_sum':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).sum().shift(c['para2']))                      \n",
    "        elif c['agg'] == 'rolling_mean':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).mean().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_max':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).max().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_min':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).min().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_median':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).median().shift(c['para2']))  \n",
    "        elif c['agg'] == 'rolling_std':    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.rolling(c['para1'],min_periods=1).std().shift(c['para2']))  \n",
    "        elif c['agg'] == 'cumcount':\n",
    "            df[new_feature] = df.groupby(c['groupby']).cumcount()   \n",
    "        elif c['agg'] == 'cumsum':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cumsum())             \n",
    "        elif c['agg'] == 'cummax':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cummax()) \n",
    "        elif c['agg'] == 'cummin':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cummin()) \n",
    "        elif c['agg'] == 'cummean':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: x.cumsum()) / (df.groupby(c['groupby']).cumcount() + 1)\n",
    "        elif c['agg'] == 'mean_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('mean') - df[c['target']]\n",
    "        elif c['agg'] == 'mean_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('mean') / (1+df[c['target']])\n",
    "        elif c['agg'] == 'trim_mean':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: stats.trim_mean(x, 0.1))             \n",
    "        elif c['agg'] == 'trim_mean_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(lambda x: stats.trim_mean(x, 0.1)) - df[c['target']]\n",
    "        elif c['agg'] == 'max_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') - df[c['target']]\n",
    "        elif c['agg'] == 'max_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') / (1+df[c['target']])   \n",
    "        elif c['agg'] == 'min_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('min')- df[c['target']]\n",
    "        elif c['agg'] == 'min_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('min')/ (1+df[c['target']])    \n",
    "        elif c['agg'] == 'max_min_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') - df.groupby(c['groupby'])[c['target']].transform('min')\n",
    "        elif c['agg'] == 'max_min_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('max') / (1+df.groupby(c['groupby'])[c['target']].transform('min'))             \n",
    "        elif c['agg'] == 'median_diff':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('median') - df[c['target']]\n",
    "        elif c['agg'] == 'median_ratio':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform('median') / (1+df[c['target']])    \n",
    "        elif c['agg'] == 'mode':\n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].apply(pd.Series.mode).reset_index(drop=True)            \n",
    "        else:    \n",
    "            df[new_feature] = df.groupby(c['groupby'])[c['target']].transform(c['agg'])\n",
    "\n",
    "agg_cols = [\n",
    "\n",
    "# ############################ aggregation##################################\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'count'},\n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'count'},\n",
    "\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'std'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'std'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_dislikes', 'agg':'mean_ratio'},    \n",
    "       \n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'original_comment_count', 'agg':'mean_ratio'},  \n",
    "    \n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'std'},    \n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'original_likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'std'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'original_dislikes', 'agg':'mean_ratio'},    \n",
    "    \n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'original_comment_count', 'agg':'mean_ratio'},    \n",
    "    \n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'min'},    \n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'std'},    \n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'original_likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'std'},\n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'original_dislikes', 'agg':'mean_ratio'},      \n",
    "    \n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'original_comment_count', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'std'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'std'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'dislikes', 'agg':'mean_ratio'},    \n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'min'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'std'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'diff_likes_dislikes', 'agg':'mean_ratio'},     \n",
    "    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['channelTitle_encoder'], 'target':'comment_count', 'agg':'mean_ratio'},  \n",
    "    \n",
    "\n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'std'},    \n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'std'},\n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'dislikes', 'agg':'mean_ratio'},    \n",
    "    \n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'std'},\n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'diff_likes_dislikes', 'agg':'mean_ratio'},     \n",
    "    \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['categoryId'], 'target':'comment_count', 'agg':'mean_ratio'},  \n",
    "\n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'min'},    \n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'std'},    \n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'std'},\n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'dislikes', 'agg':'mean_ratio'},    \n",
    "    \n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'min'},     \n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'std'},\n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'diff_likes_dislikes', 'agg':'mean_ratio'},     \n",
    "    \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'sum'},\n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'mean'},\n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'max'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'min'},     \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'std'},    \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['year'], 'target':'comment_count', 'agg':'mean_ratio'}, \n",
    "\n",
    " \n",
    "    {'groupby': ['ratings_disabled'], 'target':'likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['ratings_disabled'], 'target':'likes', 'agg':'mean_ratio'}, \n",
    "    \n",
    "    {'groupby': ['ratings_disabled'], 'target':'dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['ratings_disabled'], 'target':'dislikes', 'agg':'mean_ratio'},    \n",
    "    \n",
    "  \n",
    "    {'groupby': ['ratings_disabled'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['ratings_disabled'], 'target':'comment_count', 'agg':'mean_ratio'}, \n",
    "       \n",
    "    {'groupby': ['comments_disabled'], 'target':'likes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['comments_disabled'], 'target':'likes', 'agg':'mean_ratio'}, \n",
    "\n",
    "    {'groupby': ['comments_disabled'], 'target':'dislikes', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['comments_disabled'], 'target':'dislikes', 'agg':'mean_ratio'},    \n",
    "       \n",
    "    {'groupby': ['comments_disabled'], 'target':'comment_count', 'agg':'mean_diff'}, \n",
    "    {'groupby': ['comments_disabled'], 'target':'comment_count', 'agg':'mean_ratio'},     \n",
    "    \n",
    "   ]\n",
    "\n",
    "agg(df,agg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 s, sys: 0 ns, total: 1.89 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_cols = ['channelTitle','description','title','new_tags']\n",
    "for cols in text_cols:   \n",
    "    df[cols] = df[cols].astype(str) \n",
    " \n",
    "    df[cols +'_music'] = df[cols].apply(lambda x: 1 if 'music' in x.lower() else 0)\n",
    "    df[cols +'_official'] = df[cols].apply(lambda x: 1 if 'official' in x.lower() else 0)\n",
    "    df[cols +'_ja_official'] = df[cols].apply(lambda x: 1 if '公式' in x else 0) \n",
    "    df[cols +'_cm'] = df[cols].apply(lambda x: 1 if 'cm' in x.lower() else 0)     \n",
    "    df[cols +'_http'] = df[cols].apply(lambda x: 1 if 'http' in x.lower() else 0)    \n",
    "    df[cols +'_movie'] = df[cols].apply(lambda x: 1 if 'movie' in x.lower() else 0)    \n",
    "    df[cols +'_jp'] = df[cols].apply(lambda x: 1 if 'jp' in x.lower() else 0)     \n",
    "    df[cols +'_youtube'] = df[cols].apply(lambda x: 1 if 'youtube' in x.lower() else 0)         \n",
    "    df[cols +'_jp_movie'] = df[cols].apply(lambda x: 1 if '映画' in x else 0)      \n",
    "    df[cols +'_jp_director'] = df[cols].apply(lambda x: 1 if '監督' in x else 0)       \n",
    "    df[cols +'_jp_tohaku'] = df[cols].apply(lambda x: 1 if '東宝' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:0\n",
      "FOLD:1\n",
      "FOLD:2\n",
      "FOLD:3\n",
      "FOLD:4\n",
      "FOLD:5\n",
      "FOLD:6\n",
      "FOLD:7\n",
      "FOLD:0\n",
      "FOLD:1\n",
      "FOLD:2\n",
      "FOLD:3\n",
      "FOLD:4\n",
      "FOLD:5\n",
      "FOLD:6\n",
      "FOLD:7\n",
      "FOLD:0\n",
      "FOLD:1\n",
      "FOLD:2\n",
      "FOLD:3\n",
      "FOLD:4\n",
      "FOLD:5\n",
      "FOLD:6\n",
      "FOLD:7\n",
      "FOLD:0\n",
      "FOLD:1\n",
      "FOLD:2\n",
      "FOLD:3\n",
      "FOLD:4\n",
      "FOLD:5\n",
      "FOLD:6\n",
      "FOLD:7\n",
      "numerical features: 253 ['categoryId', 'likes', 'dislikes', 'comment_count', 'channelId_encoder', 'channelTitle_encoder', 'collection_date_encoder', 'description_encoder', 'tags_encoder', 'comments_disabled_encoder', 'ratings_disabled_encoder', 'comments_ratings_encoder', 'dislikes_pred', 'likes_pred', 'comment_count_pred', 'diff_dislikes', 'diff_likes', 'diff_comments', 'original_dislikes', 'original_likes', 'original_comment_count', 'c_month', 'c_day', 'c_dayofweek', 'year', 'month', 'weekofyear', 'day', 'dayofweek', 'hour', 'minute', 'days_from_publish', 'months_from_publish', 'years_from_publish', 'days_from_publish_start', 'days_from_cdate_start', 'likes_comments', 'dislikes_comments', 'comment_count_ratings', 'diff_likes_dislikes', 'ratio_likes_dislikes', 'ratio_likes_comment_count', 'ratio_dislikes_comment_count', 'likes_by_day', 'dislikes_by_day', 'comments_by_day', 'likes_by_month', 'dislikes_by_month', 'comments_by_month', 'likes_by_year', 'dislikes_by_year', 'comments_by_year', 'channelTitle_num_cap', 'channelTitle_num_low', 'channelTitle_num_dig', 'channelTitle_num_engdig', 'channelTitle_isja', 'channelTitle_isalpha', 'channelTitle_num_pun', 'channelTitle_num_space', 'channelTitle_num_chars', 'channelTitle_num_words', 'channelTitle_num_unique_words', 'channelTitle_ratio_unique_words', 'channelTitle_num_stopwords', 'channelTitle_num_words_upper', 'channelTitle_num_words_lower', 'channelTitle_num_words_title', 'description_num_cap', 'description_num_low', 'description_num_dig', 'description_num_engdig', 'description_isja', 'description_isalpha', 'description_num_pun', 'description_num_space', 'description_num_chars', 'description_num_words', 'description_num_unique_words', 'description_ratio_unique_words', 'description_num_stopwords', 'description_num_words_upper', 'description_num_words_lower', 'description_num_words_title', 'title_num_cap', 'title_num_low', 'title_num_dig', 'title_num_engdig', 'title_isja', 'title_isalpha', 'title_num_pun', 'title_num_space', 'title_num_chars', 'title_num_words', 'title_num_unique_words', 'title_ratio_unique_words', 'title_num_stopwords', 'title_num_words_upper', 'title_num_words_lower', 'title_num_words_title', 'new_tags_num_cap', 'new_tags_num_low', 'new_tags_num_dig', 'new_tags_num_engdig', 'new_tags_isja', 'new_tags_isalpha', 'new_tags_num_pun', 'new_tags_num_space', 'new_tags_num_chars', 'new_tags_num_words', 'new_tags_num_unique_words', 'new_tags_ratio_unique_words', 'new_tags_num_stopwords', 'new_tags_num_words_upper', 'new_tags_num_words_lower', 'new_tags_num_words_title', 'all_text_num_cap', 'all_text_num_low', 'all_text_num_dig', 'all_text_num_engdig', 'all_text_isja', 'all_text_isalpha', 'all_text_num_pun', 'all_text_num_space', 'all_text_num_chars', 'all_text_num_words', 'all_text_num_unique_words', 'all_text_ratio_unique_words', 'all_text_num_stopwords', 'all_text_num_words_upper', 'all_text_num_words_lower', 'all_text_num_words_title', 'description_url_num', 'svd_channelTitle1', 'svd_channelTitle2', 'svd_channelTitle3', 'svd_channelTitle4', 'svd_channelTitle5', 'svd_channelTitle6', 'svd_channelTitle7', 'svd_channelTitle8', 'svd_channelTitle9', 'svd_channelTitle10', 'svd_channelTitle11', 'svd_channelTitle12', 'svd_channelTitle13', 'svd_channelTitle14', 'svd_channelTitle15', 'svd_channelTitle16', 'svd_channelTitle17', 'svd_channelTitle18', 'svd_channelTitle19', 'svd_channelTitle20', 'svd_title1', 'svd_title2', 'svd_title3', 'svd_title4', 'svd_title5', 'svd_title6', 'svd_title7', 'svd_title8', 'svd_title9', 'svd_title10', 'svd_title11', 'svd_title12', 'svd_title13', 'svd_title14', 'svd_title15', 'svd_title16', 'svd_title17', 'svd_title18', 'svd_title19', 'svd_title20', 'svd_all_text1', 'svd_all_text2', 'svd_all_text3', 'svd_all_text4', 'svd_all_text5', 'svd_all_text6', 'svd_all_text7', 'svd_all_text8', 'svd_all_text9', 'svd_all_text10', 'svd_all_text11', 'svd_all_text12', 'svd_all_text13', 'svd_all_text14', 'svd_all_text15', 'svd_all_text16', 'svd_all_text17', 'svd_all_text18', 'svd_all_text19', 'svd_all_text20', 'svd_char_new_tags1', 'svd_char_new_tags2', 'svd_char_new_tags3', 'svd_char_new_tags4', 'svd_char_new_tags5', 'svd_char_new_tags6', 'svd_char_new_tags7', 'svd_char_new_tags8', 'svd_char_new_tags9', 'svd_char_new_tags10', 'svd_char_new_tags11', 'svd_char_new_tags12', 'svd_char_new_tags13', 'svd_char_new_tags14', 'svd_char_new_tags15', 'svd_char_new_tags16', 'svd_char_new_tags17', 'svd_char_new_tags18', 'svd_char_new_tags19', 'svd_char_new_tags20', 'channelTitle_music', 'channelTitle_official', 'channelTitle_ja_official', 'channelTitle_cm', 'channelTitle_http', 'channelTitle_movie', 'channelTitle_jp_movie', 'channelTitle_jp_director', 'channelTitle_jp_tohaku', 'description_music', 'description_official', 'description_ja_official', 'description_cm', 'description_http', 'description_movie', 'description_jp_movie', 'description_jp_director', 'description_jp_tohaku', 'title_music', 'title_official', 'title_ja_official', 'title_cm', 'title_http', 'title_movie', 'title_jp_movie', 'title_jp_director', 'title_jp_tohaku', 'new_tags_music', 'new_tags_official', 'new_tags_ja_official', 'new_tags_cm', 'new_tags_http', 'new_tags_movie', 'new_tags_jp_movie', 'new_tags_jp_director', 'new_tags_jp_tohaku', 'categoryId_target_mean', 'ratings_disabled_target_mean', 'comments_disabled_target_mean', 'comments_ratings_disabled_target_mean']\n",
      "FOLD:0\n",
      "train_x shape: (17255, 253) 12.526670233909586\n",
      "valid_x shape: (2465, 253) 12.540542561773018\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's rmse: 0.65023\tvalid_0's rmse: 0.800837\n",
      "[1000]\ttraining's rmse: 0.53828\tvalid_0's rmse: 0.773009\n",
      "[1500]\ttraining's rmse: 0.475684\tvalid_0's rmse: 0.765064\n",
      "[2000]\ttraining's rmse: 0.426283\tvalid_0's rmse: 0.759892\n",
      "[2500]\ttraining's rmse: 0.385157\tvalid_0's rmse: 0.756541\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import catboost as cat\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import svm, neighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "def preprocess(train_df,test_df,feats):\n",
    "    train_df = train_df.replace([np.inf, -np.inf], np.nan)\n",
    "    train_df = train_df.fillna(0) \n",
    "\n",
    "    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n",
    "    test_df = test_df.fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_df[feats] = scaler.fit_transform(train_df[feats])\n",
    "    test_df[feats] = scaler.transform(test_df[feats])\n",
    "    \n",
    "    return train_df[feats], test_df[feats]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))** .5\n",
    "\n",
    "def target_encoder_kfold(train_df,test_df,col,target,folds,method):\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_df['y_bin'])):\n",
    "        print ('FOLD:' + str(n_fold))\n",
    "        train_x = train_df.iloc[train_idx]\n",
    "        valid_x = train_df.iloc[valid_idx] \n",
    "        if method == 'mean':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].apply(lambda x: stats.trim_mean(x, 0.01)))\n",
    "        if method == 'median':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].median())   \n",
    "        if method == 'max':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].max())  \n",
    "        if method == 'min':\n",
    "            oof_preds[valid_idx] = valid_x[col].map(train_x.groupby(col)[target].min())              \n",
    "    if method == 'mean':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].apply(lambda x: stats.trim_mean(x, 0.01)))\n",
    "    if method == 'median':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].median())\n",
    "    if method == 'max':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].max())\n",
    "    if method == 'min':    \n",
    "        sub_preds = test_df[col].map(train_df.groupby(col)[target].min())        \n",
    "    return oof_preds,sub_preds\n",
    "\n",
    "\n",
    "def lgb_kfold(train_df,test_df,features,target,cat_features,folds,params,use_pseudo=False,sampling=False):\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "    cv_list = []\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[features], train_df['y_bin'])):\n",
    "        print ('FOLD:' + str(n_fold))\n",
    "        \n",
    "        train_x, train_y = train_df[features].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "        \n",
    "        # remove outliers\n",
    "        if sampling is True:\n",
    "            valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx] \n",
    "            train_df_new = train_df.iloc[train_idx]\n",
    "            train_df_new = train_df_new[train_df_new['y_residual']<3.2]#3\n",
    "            train_x, train_y = train_df_new[features], train_df_new[target]\n",
    "            \n",
    "       # pseudo \n",
    "        if use_pseudo is True:\n",
    "            train_x = pd.concat([train_x ,pseudo[features]],axis=0)\n",
    "            train_y = train_y.append(pseudo[target])  \n",
    "            for n, (pseudo_train_idx, pseudo_valid_idx) in enumerate(folds.split(pseudo[features], pseudo['y_bin'])):\n",
    "                print ('PSEUDO FOLD:' + str(n))\n",
    "                if n_fold == n:\n",
    "                    train_x = pd.concat([train_x ,pseudo[features].iloc[pseudo_valid_idx]],axis=0)\n",
    "                    train_y = train_y.append(pseudo[target].iloc[pseudo_valid_idx] )\n",
    "                    break\n",
    "                    \n",
    "        print ('train_x shape:',train_x.shape,train_y.mean())\n",
    "        print ('valid_x shape:',valid_x.shape,valid_y.mean())\n",
    "        \n",
    "        dtrain = lgb.Dataset(train_x, label=train_y,categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features) \n",
    "        bst = lgb.train(params, dtrain, num_boost_round=50000,\n",
    "            valid_sets=[dval,dtrain], verbose_eval=500,early_stopping_rounds=500, ) \n",
    "        new_list = sorted(zip(features, bst.feature_importance('gain')),key=lambda x: x[1], reverse=True)[:30]\n",
    "        for item in new_list:\n",
    "            print (item) \n",
    "         \n",
    "        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "        oof_cv = rmse(valid_y,  oof_preds[valid_idx])\n",
    "        cv_list.append(oof_cv)\n",
    "        print (cv_list)\n",
    "        sub_preds += bst.predict(test_df[features], num_iteration=bst.best_iteration) / folds.n_splits\n",
    " \n",
    "    cv = rmse(train_df[target],  oof_preds)\n",
    "    print('Full OOF RMSE %.6f' % cv)  \n",
    "\n",
    "    train_df['lgb_y'] = oof_preds\n",
    "    test_df['lgb_y'] = sub_preds\n",
    "    \n",
    "    return train_df,test_df,cv\n",
    "\n",
    "params = {\n",
    "               \"objective\" : \"regression\", \n",
    "               \"boosting\" : \"gbdt\", \n",
    "               \"metric\" : \"rmse\",  \n",
    "               \"max_depth\": -1,\n",
    "               \"min_data_in_leaf\": 10, #10\n",
    "               \"min_gain_to_split\": 0.01,#0.01\n",
    "                \"min_child_weight\": 0.001,#0.001\n",
    "                \"reg_alpha\": 0.1, #0.1\n",
    "                \"reg_lambda\": 1, #1\n",
    "               \"num_leaves\" : 35, #40\n",
    "               \"max_bin\" : 300,#300 \n",
    "              \"learning_rate\" :0.01,\n",
    "               \"bagging_fraction\" : 0.9,\n",
    "               \"bagging_freq\" : 1,\n",
    "               \"bagging_seed\" : 4590,\n",
    "               \"feature_fraction\" : 0.6,#0.6\n",
    "               \"verbosity\": -1,\n",
    "               \"boost_from_average\": False,\n",
    "}\n",
    "\n",
    "\n",
    "df2 = df.copy()\n",
    "train_df = df2[df2['y'].notnull()] \n",
    "train_df['y'] = np.log1p(train_df['y'])\n",
    "test_df = df2[df2['y'].isnull()] \n",
    "\n",
    "drop_features=[ 'channelId', 'channelTitle', 'c_date','collection_date','image','comments_ratings_disabled',\n",
    "        'description', 'ratio_title_des','comments_disabled','ratings_disabled',\n",
    "       'id',  'publishedAt',  'tags','new_tags','all_text','new_title', 'new_description',\n",
    "       'thumbnail_link', 'title', 'video_id', 'y','y_bin','lgb_y','comments_ratings',\n",
    " \n",
    " 'month_sum_likes', 'month_mean_likes', 'month_max_likes', 'month_min_likes', 'month_std_likes', 'month_mean_diff_likes', 'month_mean_ratio_likes', \n",
    "               'month_sum_dislikes', 'month_mean_dislikes', 'month_max_dislikes', 'month_min_dislikes', 'month_std_dislikes', 'month_mean_diff_dislikes',\n",
    "               'month_mean_ratio_dislikes', 'month_sum_comment_count', 'month_mean_comment_count', 'month_max_comment_count', 'month_min_comment_count', \n",
    "               'month_std_comment_count', 'month_mean_diff_comment_count', 'month_mean_ratio_comment_count',\n",
    "               'dayofweek_sum_likes', 'dayofweek_mean_likes', 'dayofweek_max_likes', 'dayofweek_min_likes', 'dayofweek_std_likes', 'dayofweek_mean_diff_likes', \n",
    "               'dayofweek_mean_ratio_likes', 'dayofweek_sum_dislikes', 'dayofweek_mean_dislikes', 'dayofweek_max_dislikes', 'dayofweek_min_dislikes', 'dayofweek_std_dislikes',\n",
    "               'dayofweek_mean_diff_dislikes', 'dayofweek_mean_ratio_dislikes', 'dayofweek_sum_comment_count', 'dayofweek_mean_comment_count', 'dayofweek_max_comment_count', 'dayofweek_min_comment_count', \n",
    "               'dayofweek_std_comment_count', 'dayofweek_mean_diff_comment_count', 'dayofweek_mean_ratio_comment_count', \n",
    "               'hour_sum_likes', 'hour_mean_likes', 'hour_max_likes', 'hour_min_likes', 'hour_std_likes', 'hour_mean_diff_likes', 'hour_mean_ratio_likes',\n",
    "               'hour_sum_dislikes', 'hour_mean_dislikes', 'hour_max_dislikes', 'hour_min_dislikes', 'hour_std_dislikes', 'hour_mean_diff_dislikes',\n",
    "               'hour_mean_ratio_dislikes', 'hour_sum_comment_count', 'hour_mean_comment_count', 'hour_max_comment_count', 'hour_min_comment_count',\n",
    "               'hour_std_comment_count', 'hour_mean_diff_comment_count', 'hour_mean_ratio_comment_count',                \n",
    "'seconds_from_publish',  \n",
    "'c_year',\n",
    "'categoryId_min_likes', \n",
    "'categoryId_min_dislikes', \n",
    "'categoryId_min_comment_count', \n",
    "'year_min_likes', \n",
    "'year_min_dislikes', \n",
    "'year_min_comment_count',            \n",
    "'residual',\n",
    "               'svd_description1', 'svd_description2', 'svd_description3', 'svd_description4', 'svd_description5', \n",
    "               'svd_description6', 'svd_description7', 'svd_description8', 'svd_description9', 'svd_description10',\n",
    "               'svd_description11', 'svd_description12', 'svd_description13', 'svd_description14', 'svd_description15',\n",
    "               'svd_description16', 'svd_description17', 'svd_description18', 'svd_description19', 'svd_description20',    \n",
    "\n",
    "               #'channelTitle_official', 'channelTitle_ja_official', \n",
    "   \n",
    "               #'channelTitle_movie',\n",
    "               'channelTitle_jp', 'channelTitle_youtube', \n",
    "#                'channelTitle_jp_movie', 'channelTitle_jp_director', 'channelTitle_jp_tohaku',\n",
    "    \n",
    "               #'description_official', 'description_ja_official', \n",
    "         \n",
    "       \n",
    "               #'description_movie', \n",
    "               'description_jp', 'description_youtube', \n",
    "#                'description_jp_movie', 'description_jp_director', 'description_jp_tohaku', \n",
    "        \n",
    "               #'title_official', 'title_ja_official', \n",
    "           \n",
    "       \n",
    "#                'title_movie', \n",
    "               'title_jp', 'title_youtube', \n",
    "               #'title_jp_movie', 'title_jp_director', \n",
    "#                'title_jp_tohaku', \n",
    "            \n",
    "               #'new_tags_official', 'new_tags_ja_official', \n",
    "            \n",
    "          \n",
    "               #'new_tags_movie',\n",
    "               'new_tags_jp', 'new_tags_youtube',\n",
    "#                'new_tags_jp_movie', 'new_tags_jp_director', 'new_tags_jp_tohaku',                                \n",
    "              ]\n",
    "\n",
    "seed = 817\n",
    "folds = StratifiedKFold(n_splits=8, shuffle=True, random_state=seed)\n",
    "target = 'y'\n",
    "\n",
    "# target encoder\n",
    "train_df['categoryId_target_mean'],test_df['categoryId_target_mean'] = target_encoder_kfold(train_df,test_df,'categoryId',target,folds,'mean')     \n",
    "train_df['ratings_disabled_target_mean'],test_df['ratings_disabled_target_mean'] = target_encoder_kfold(train_df,test_df,'ratings_disabled',target,folds,'mean')  \n",
    "train_df['comments_disabled_target_mean'],test_df['comments_disabled_target_mean'] = target_encoder_kfold(train_df,test_df,'comments_disabled',target,folds,'mean') \n",
    "train_df['comments_ratings_disabled_target_mean'],test_df['comments_ratings_disabled_target_mean'] = target_encoder_kfold(train_df,test_df,'comments_ratings_disabled',target,folds,'mean') \n",
    "\n",
    " \n",
    "features = [f for f in train_df.columns if f not in drop_features]\n",
    "\n",
    "cat_features = [\n",
    "    'categoryId',\n",
    "    'channelTitle_encoder', \n",
    "    'comments_ratings_encoder',\n",
    "    'c_month', \n",
    "    'year', \n",
    "]\n",
    "\n",
    "print ('numerical features:', len(features),features)# \n",
    "\n",
    "train_lgb,test_lgb,cv = lgb_kfold(train_df,test_df,features,target,cat_features,folds,params,use_pseudo=False,sampling=False)\n",
    "\n",
    "out_dir = \"out_tmp\"\n",
    "!mkdir -p $out_dir\n",
    "\n",
    "# submission\n",
    "train_lgb[['id','lgb_y','y']].to_csv(f'./{out_dir}/train_lgb_0624.csv',index=False)\n",
    "test_lgb[['id','lgb_y']].to_csv(f'./{out_dir}/test_lgb_0624.csv',index=False)\n",
    "\n",
    "display(train_lgb[['y','lgb_y']].describe())\n",
    "display(test_lgb['lgb_y'].describe())\n",
    "test_lgb['y'] = np.expm1(test_lgb['lgb_y'])\n",
    "display(test_lgb['y'].describe())\n",
    "test_lgb[['id','y']].to_csv(f'./{out_dir}/sub_lgb/sub_0624_{cv}.csv',index=False)\n",
    "test_lgb[['id','y']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
